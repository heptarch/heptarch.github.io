--------

This is the home page for Xanadu's quantum machine learning
journal club. We will be working through an idiosyncratic syllabus on
classical ML, in the hopes it will contribute to our expertise in QML,
offer new insights or research approaches, and generally enhance our quality of life.

#+ATTR_HTML: :alt ML image :align center :width 400px :style display:inline;margin:-20px;
[[./img/qml/ML.png]]

Here is the schedule (to be populated):

| Date         | Material      | Guide              | Scribe |
|--------------+---------------+--------------------+--------|
| [[week0][Jan 9]], 2024  | Admin         | David Wakeham (DW) | DW     |
| [[week1][Jan 16]], 2024 | ESL: ยง2.1-2.4 | DW                 | DW     |
|              | (SVM: ยง1.1)   |                    |        |
|--------------+---------------+--------------------+--------|

Notes on past meetings are in chronological order below.

* <<week0>> Admin
-----

*Reading*: None.

Based on popular accord, we'll be focusing on the classical
statistical approach to ML embodied in [[https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf][/Elements of Statistical Learning/]], and the
effective field theory paradigm of [[https://arxiv.org/pdf/2106.10165.pdf][/The Principles of Deep
Learning/]]. These are like the out-of-field examiner and the scary
departmental theorist at your viva. These foci will not stop us from being
eclectic, and dipping into resources on [[https://arxiv.org/pdf/2104.13478.pdf][/Geometric Deep Learning/]],
[[https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf][/Support Vector Machines/]] or [[https://www.deeplearningbook.org/][/Deep Learning/]] proper, as time, interest, and chunking limits permit.

Each week we will have designated reading, optional readings (in
parentheses in the schedule), and a guide or guides who have agreed (or been
voluntold) to talk us through the readings. There will
also be a scribe to take notes. Notes will be uploaded here. For any
corrections, email [[mailto:7@heptar.ch][~<7@heptar.ch>~]].

* <<week1>> Introduction
-----

** Statistical learning theory in brief

*Reading*: [[SVM][SVM]]: ยง1.1.

In statistical learning theory, our goal is to optimally approximate a
function $X \to Y$ using a /training set/ of input-output samples,
\[
\mathcal{D} = \{(x_{(1)}, y_{(1)}), \ldots, (x_{(n)}, y_{(n)})\}.
\]
We assume, further, that these are independently generated according to
\[
P(x, y) = P_X(x) P(y|x),
\]
in other words, we choose $x$ via the marginal distribution
$\mathbb{P}_X(\cdot)$ and $y$ via the conditional distribution
$\mathbb{P}(\cdot|x)$.
This latter distribution accomodates noise and stochasticity in the
function $f$.
We assume both distributions are unknown.@@html:
<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
In particular, we're not doing <i>parameteric</i> estimation.
	   </span>
@@

For simplicity, suppose $Y \subseteq \mathbb{R}$.
A /loss function/ is a map $L: X \times Y \times \mathbb{R} \to
[0, \infty)$ which measures how well $f$ interpolates the data point
$(x, y)$, with smaller values of $L(x, y, f(x))$ as $f(x)$ gets closer
to $y$.
The /expected loss/ or /risk/ for a given $f: X\to Y$, over the joint
distribution $P(x, y)$, is
\[
\mathcal{R}(f) = \mathbb{E}[L(x, y, f(x))] = \int_{X\times Y} L(x, y, f(x))\, \mathrm{d}P(x,
y). \tag{2.1} \label{risk}
\]
This is the average loss when we select an infinite number of pairs
according to $P(x, y)$. The /empirical risk/ is the empirical
approximation using our training pairs,
\[
\mathcal{R}_{\mathcal{D}}(f) = \frac{1}{n}\sum_{i=1}^n L(x_{(i)},
y_{(i)}, f(x_{(i)})). \tag{2.2} \label{emp-risk}
\]
As $n \to \infty$, the law of large numbers implies that
$\mathcal{R}_{\mathcal{D}} \to \mathcal{R}$, i.e. the empirical risk
converges in probability to the risk. So for many data points, we
can approximate this expected loss well.
How do we learn the approximating function $f$ based on these ideas?
The minimal risk for a given loss function and distribution is
\[
\mathcal{R}^* = \inf_{f: X \to Y} \mathcal{R}(f).
\]
/Learning methods/ uniquely associate functions to training data,
$\mathcal{D} \mapsto f_{\mathcal{D}}$. The hope is that they
approximate the minimal risk, $\mathcal{R}(f_{\mathcal{D}}) \approx
\mathcal{R}^*$, at least as our training set gets asymptotically
large. More formally, we say a learning method is /universally
consistent/ if
\[
\lim_{n\to \infty} \mathcal{R}(f_{\mathcal{D}_n}) = \mathcal{R}^*,
\]
i.e. the function we learn minimizes risk.

In some ways, it's surprising such methods can exist for arbitrary
distributions $P$.
In another way, it isn't, because we're only requiring asymptotic
consistency, so we can gather enough data to map out $P(x, y)$ to
arbitrary accuracy.
And indeed, the /no free lunch (NFL) theorem/ states that, for any
given speed of convergence (specified by a decreasing sequence), and
any universally consistent learning method, there is some probability
distribution which cannot be learned that quickly.

** Linear and nonlinear regression

*Reading*: [[ESL][ESL]]: ยง2.3.

This is quite high concept, so let's dive into some specific methods
for prediction, which also give us a sense of how statisticians think.
One of the important tools in statistics is /linear models/ aka
/linear regression/.
In the case our domain $X = \mathbb{R}^p$, and $Y = \mathbb{R}$, and
we have reason to the inputs and outputs are linearly related, we can
write
\[
f(x) = \hat{\beta}_0 + \sum_{j=1}^p x_j\hat{\beta}_j = x^T \hat{\beta}
\tag{2.3} \label{linmod}
\]
where $x_j$ is the $j$-th component of the vector $x$, and in the
second equation, we have padded out $x$ with a $1$ in the zeroth
component.
The coefficients $\hat{\beta}$ form a vector of $p+1$ parameters (with
the hat reminding us they are to be estimated from data), but we
can extend this to a matrix for $Y = \mathbb{R}^k$.
For the moment, let's keep $k=1$, and absorb the padding into $p$.

As before, we would like to pick the best $\beta$ for our data using a
loss function. A natural and in some sense optimal choice for linear
models is called the /residual sum of squares/, where the loss
function is just squared distance:
\[
L(x, y, f(x)) = |y - f(x)|^2 = |y - x^T \beta|^2.
\]
Let $\mathbf{y} = (y_{(i)})^T$ be a column vector of $n$ training
outputs, and $\mathbf{X} = (x_{(i)}^T)$ an $n\times p$ matrix of
training inputs.
The empirical risk $(\ref{emp-risk})$ can then be written
\[
\mathcal{R}_{\mathcal{D}}(\beta) = \frac{1}{n}\sum_{i=1}|y_{(i)} -
x_{(i)}^T\beta|^2 = \frac{1}{n}(\mathbf{y} - \mathbf{X}\beta)^T
(\mathbf{y} - \mathbf{X}\beta). \tag{2.4} \label{RSS}
\]
We can solve this for the optimal $\beta$, simply by differentiating
with respect to $\beta$:
\[
\partial_\beta \mathcal{\mathcal{R}_{\mathcal{D}}(\beta)}
= \frac{1}{n}\left[\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}
\right) + \left(\mathbf{X}\beta -\mathbf{y} \right)^T
\mathbf{X}\right] = \frac{2}{n}\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}\right),
\]
where the last equality follows from the fact that a scalar is its own transpose.
More carefully, we can differentate either component-wise, or with
respect to both $\beta$ and $\beta^T$ as formal variables. Assuming
$\mathbf{X}^T\mathbf{X}$ has an inverse, we can set this to zero and
solve for $\beta$:
\[
\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}\right) =
\mathbf{X}^T\mathbf{X}\beta -\mathbf{X}^T\mathbf{y} = 0 \quad
\Longrightarrow \quad \hat{\beta} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}. \tag{2.5} \label{beta-hat}
\]
Again, the hat tells us that we estimate this from data.

Let's look at an example.
Below, we have a scatterplot of $100$ simulated data points on a pair
of inputs, $x = (x_1, x_2)$. The output is a categorical
binary variable, i.e. $0$ and $1$, indicated by
$\textcolor{blue}{\texttt{o}}$ and $\textcolor{orange}{\texttt{o}}$ in
the scatterplot.
We can fit a linear regression model to the data, with a real variable
$y$ which gets coded as $0$ when $y \leq 0.5$ and $1$ otherwise. In
other words, we have a linear decision boundary at $y = x^T\hat{\beta}
= 0.5$, where $\hat{\beta}$ is given by $(\ref{beta-hat})$.
#+ATTR_HTML: :alt Linear classifier for random data. image :align center :width 300px :style display:inline;margin:-30px;
[[./img/qml/esl-intro1.png]]

There are misclassifications, but whether our model is good or bad
depends strongly on our assumptions about the data. Consider two
possibilities:
- The data comes from two independent bivariate Gaussian and different
  means.
- The data comes from a mixture of $10$ low-variance Gaussians, with
  means distributed in a Gaussian fashion.

In the first case, the data is, in fact, optimally described by a
linear decision boundary (bisecting the means of the component
Gaussians) but inevitably noisy. In the second case, we have clusters,
so a linear decision boundary will do poorly on future data.
Let's turn to a nonlinear method that does better in this second scenario.

/Nearest neighbours/ is a very different approach. The idea is that is
simply to approximate the value of response variables for a new data
point by averaging the values of its nearest neighbours in the training data
$\mathcal{D}$. Let $N_k(x)$ denote the $k$ nearest neighbours of
$x$. Then our /$k$-nearest-neighbour regressor/ is
\[
f(x) = \frac{1}{k}\sum_{x_{(i)} \in N_k(x)} y_{(i)}.
\]
Closeness will mean in Euclidean distance, but could of course be
generalized to an arbitrary metric space.
In our binary case, this boils down to a majority vote.
This is a much less rigid method than linear regression, so when we
fit our binary-coded data, we get a much wobblier decision boundary
$f(x) = 0.5$.
Below, we show $k = 15$:
#+ATTR_HTML: :alt Nearest neighbors for random data. image :align center :width 300px :style display:inline;margin:-30px;
[[./img/qml/esl-intro2.png]]

We have fewer misclassifications, but of course, we can easily
overfit with nearest neighbours. For instance, with $k=1$ there are
/no/ misclassifications, by definition! So to
determine the appropriate choice of $k$, we need a test set and a
method besides minimizing empirical risk.
Note that to count parameters, we should use $n/k$ (where $n
= |\mathcal{D}|$) rather than $k$, since this is the number of
clusters it gloms our data into.

To get test data, we need to open up the data-generating black box.
The data is generated hierarchically as follows: we have two Gaussian
distributions of Gaussians, with ten means drawn from
one distribution (centred at $(1, 0)^T$) and ten from another (centred
at $(0, 1)^T$), forming our two different classes
($\textcolor{blue}{\texttt{o}}$ and
$\textcolor{orange}{\texttt{o}}$). Each mean had a small
Gaussian centered on it, and from the two classes, Gaussians were chosen at
random to draw observations from.
We can see how methods perform on a test set:

#+ATTR_HTML: :alt Misclassification curves. image :align center :width 400px :style display:inline;margin:-30px;
[[./img/qml/esl-intro3.png]]

The purple line is the optimal Bayes error (to be discussed below).
The two large squares are the results of linear regression, which has
three degrees of freedom.
The curves are results for nearest neighbours, with $n/k$ increasing
to the right.
Interestingly, it performs as well as linear regression for the same
number of parameters, with lower training error.
How can we get a handle on these two methods?

** Regression and statistical learning

*Reading*: [[ESL][ESL]]: ยง2.4.

Let's return to the statistical learning theory we began
to explore above, but with our two examples in mind.
We will focus on squared error loss in the risk $(\ref{risk})$. If we
condition on $x$, using the definition of conditional probability $P(x, y) = P(y|x)P(x)$, then
\[
\mathcal{R}(f) = \mathbb{E}[L_2(x, y, f(x))] = \mathbb{E}[(x - f(y))^2] = \mathbb{E}_x\big[
\mathbb{E}_{y|x} [(y - f(x))^2 | x]\big].
\]
To minimize the expectation of a function over $x$, it suffices to
minimize it /pointwise/, i.e. for each $x$, we want to choose
\[
f(x) = \mbox{argmin}_c \mathbb{E}_{y|x} [(y - c)^2 | x].
\]
To minimize this, note that
\[
\partial_c\mathbb{E}_{y|x} [(y - c)^2 | x] = \partial_c\int_{y\in Y}
(y-c)^2 \,\mathrm{d}P(y|x) = 2\int_{y\in Y}(c-y) \,\mathrm{d}P(y|x) =
2c - 2\mathbb{E}_y[y|x].
\]
Setting this to zero, we should take
\[
f(x) = \mathbb{E}_y[y|x]. \tag{2.6} \label{regress-f}
\]
This is called the /regression function/, and it tells us that to
minimize expected squared loss, we should choose the conditional
mean.
Of course, we could use a different loss function, say absolute value
\[
L_1(x, y, f(y)) = |y - f(x)|
\]
instead, and the same argument would give the /conditional median/
\[
f(x) = \mbox{argmin}_c \mathbb{E}_{y|x}[|y - c|\, | x].
\]
However, we'll continue focusing on squared loss.

In a sense, both linear regression and nearest-neighbours try to
approximate the conditional mean.
Nearest neighbours approximates this mean in two ways. First, it uses empirical
probabilities rather than the underlying distribution, since it only
has access to the latter:
\[
\mathbb{E}_y[y|x] \approx \sum_{(x, y) \in \mathcal{D}} y.
\]
Second, since data is usually sparse, we need to replace conditioning
on $x$ for conditioning on a neighbourhood of $x$:
\[
\mathbb{E}_y[y|x] \approx \sum_{(x, y) \in \mathcal{D}} y.
\]

* References
-----
1. <<ESL>> [[https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf][/Elements of Statistical Learning/]] (2008), Hastie, 
   Tibshirani and Friedman. [ESL]
2. <<PDL>> [[https://arxiv.org/pdf/2106.10165.pdf][/The Principles of Deep Learning/]] (2021), Roberts and 
   Yaida. [PDL]
3. <<GDL>> [[https://arxiv.org/pdf/2104.13478.pdf][/Geometric Deep Learning/]] (2021), Bronstein, Bruna,
   Cohen, and Veliฤkoviฤ. [GDL]
4. <<SVM>> [[https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf][/Support Vector Machines/]] (2008), Steinwart and Christmann. [SVM]
5. <<DLB>> [[https://www.deeplearningbook.org/][/Deep Learning/]] (2015), Aaron Courville, Ian Goodfellow,
   and Yoshua Bengio. [DLB]
* COMMENT html export
#+CREATOR: 
#+AUTHOR: 
#+TITLE:
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: xhtml-strict
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style2.css" ><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1><b>QML journal club</b></h1>
#+HTML_LINK_HOME:
#+HTML_LINK_UP:
#+HTML_MATHJAX:
#+INFOJS_OPT:
#+LATEX_HEADER:
#+OPTIONS: html-postamble:nil
