--------

This is the home page for Xanadu's quantum machine learning
journal club. We will be working through an idiosyncratic syllbaus on
classical ML, in the hopes it will contribute to our expertise in QML,
offer new insights or research approaches, and generally enhance our quality of life.

#+ATTR_HTML: :alt ML image :align center :width 400px :style display:inline;margin:-20px;
[[./img/qml/ML.png]]

Here is the schedule (to be populated):

| Date         | Material         | Guide         | Scribe        |
|--------------+------------------+---------------+---------------|
| [[week0][Jan 9]], 2024  | Admin, SVM: §1.1 | David Wakeham | David Wakeham |
| [[week1][Jan 16]], 2024 | ESL: §2.1-2.4    | TBA           | TBA           |
|--------------+------------------+---------------+---------------|

Notes on past meetings are in chronological order below.

* <<week0>> Admin
-----

*Reading*: None. *Homework*: None.

Based on popular accord, we'll be focusing on the classical
statistical approach to ML embodied in [[https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf][/Elements of Statistical Learning/]], and the
effective field theory paradigm of [[https://arxiv.org/pdf/2106.10165.pdf][/The Principles of Deep
Learning/]]. These are like the out-of-field examiner and the scary
departmental theorist at your viva. These foci will not stop us from being
eclectic, and dipping into resources on [[https://arxiv.org/pdf/2104.13478.pdf][/Geometric Deep Learning/]],
[[https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf][/Support Vector Machines/]] or [[https://www.deeplearningbook.org/][/Deep Learning/]] proper, as time, interest, and chunking limits permit.

Each week we will have designated reading, optional readings (in
parentheses in the schedule), and a guide or guides who have agreed (or been
voluntold) to talk us through the readings. There will
also be a scribe to take notes (in [[https://orgmode.org/worg/org-syntax.html][~org mode~]] or anything else
[[https://pandoc.org/][~Pandoc~]] can handle). Notes will be hosted here at
[[https://heptar.ch/qml-jc][~heptar.ch/qml-jc~]], and maintained by David Wakeham. For any
corrections, email [[mailto:7@heptar.ch][~<7@heptar.ch>~]].

Finally, to ensure it is not only the machines learning, we will try
to do a healthy number of homework problems. The precise number and
assignment mechanism will be determined collaboratively.

* <<week1>> Overview of supervised learning
-----

** Statistical learning theory in brief

*Reading*: [[SVM][SVM]]: §1.1.

In statistical learning theory, our goal is to optimally approximate a
function $X \to Y$ using a /training set/ of input-output samples,
\[
\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}.
\]
We assume, further, that these are independently generated according to
\[
P(x, y) = P_X(x) P(y|x),
\]
in other words, we choose $x$ via the marginal distribution
$\mathbb{P}_X(\cdot)$ and $y$ via the conditional distribution
$\mathbb{P}(\cdot|x)$.
This latter distribution accomodates noise and stochasticity in the
function $f$.
We assume both distributions are unknown.@@html:
<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
In particular, we're not doing <i>parameteric</i> estimation.
	   </span>
@@

For simplicity, suppose $Y \subseteq \mathbb{R}$.
A /loss function/ is a map $L: X \times Y \times \mathbb{R} \to
[0, \infty)$ which measures how well $f$ interpolates the data point
$(x, y)$, with smaller values of $L(x, y, f(x))$ as $f(x)$ gets closer
to $y$.
The /expected loss/ or /risk/ for a given $f: X\to Y$, over the joint
distribution $P(x, y)$, is
\[
\mathcal{R}(f) = \int_{X\times Y} L(x, y, f(x))\, \mathrm{d}P(x, y).
\]
This is the average loss when we select an infinite number of pairs
according to $P(x, y)$. The *empirical risk* is the empirical
approximation using our training pairs,
\[
\mathcal{R}_{\mathcal{D}}(f) = \frac{1}{n}\sum_{i=1}^n L(x_i, y_i, f(x_i)).
\]
As $n \to \infty$, the law of large numbers implies that
$\mathcal{R}_{\mathcal{D}} \to \mathcal{R}$, i.e. the empirical risk
converges in probability to the risk. So for many data points, we
can approximate this expected loss well.

How do we learn the approximating function $f$ based on these ideas?
The minimal risk for a given loss function and distribution is
\[
\mathcal{R}^* = \inf_{f: X \to Y} \mathcal{R}(f).
\]
/Learning methods/ uniquely associate functions to training data,
$\mathcal{D} \mapsto f_{\mathcal{D}}$. The hope is that they
approximate the minimal risk, $\mathcal{R}(f_{\mathcal{D}}) \approx
\mathcal{R}^*$, at least as our training set gets asymptotically
large. More formally, we say a learning method is *universally
consistent* if
\[
\lim_{n\to \infty} \mathcal{R}(f_{\mathcal{D}_n}) = \mathcal{R}^*,
\]
i.e. the function we learn minimizes risk.

In some ways, it's surprising such methods can exist for arbitrary
distributions $P$.
In another way, it isn't, because we're only requiring asymptotic
consistency, so we can gather enough data to map out $P(x, y)$ to
arbitrary accuracy.
And indeed, the /no free lunch (NFL) theorem/ states that, for any
given speed of convergence (specified by a decreasing sequence), and
any universally consistent learning method, there is some probability
distribution which cannot be learned that quickly.

* References
-----
1. <<ESL>> [[https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf][/Elements of Statistical Learning/]] (2008), Hastie, 
   Tibshirani and Friedman. [ESL]
2. <<PDL>> [[https://arxiv.org/pdf/2106.10165.pdf][/The Principles of Deep Learning/]] (2021), Roberts and 
   Yaida. [PDL]
3. <<GDL>> [[https://arxiv.org/pdf/2104.13478.pdf][/Geometric Deep Learning/]] (2021), Bronstein, Bruna,
   Cohen, and Veličković. [GDL]
4. <<SVM>> [[https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf][/Support Vector Machines/]] (2008), Steinwart and Christmann. [SVM]
5. <<DLB>> [[https://www.deeplearningbook.org/][/Deep Learning/]] (2015), Aaron Courville, Ian Goodfellow,
   and Yoshua Bengio. [DLB]
* COMMENT html export
#+CREATOR: 
#+AUTHOR: 
#+TITLE:
#+HTML_CONTAINER: div
#+HTML_DOCTYPE: xhtml-strict
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style2.css" ><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1><b>QML journal club</b></h1>
#+HTML_LINK_HOME:
#+HTML_LINK_UP:
#+HTML_MATHJAX:
#+INFOJS_OPT:
#+LATEX_HEADER:
#+OPTIONS: html-postamble:nil
