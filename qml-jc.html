<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title></title>
<!-- 2024-01-20 Sat 12:25 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="style2.css" ><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1><b>QML journal club</b></h1>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title"></h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Admin</a></li>
<li><a href="#sec-2">2. Overview of supervised learning</a>
<ul>
<li><a href="#sec-2-1">2.1. Statistical learning theory in brief</a></li>
<li><a href="#sec-2-2">2.2. Linear regression</a></li>
</ul>
</li>
<li><a href="#sec-3">3. References</a></li>
</ul>
</div>
</div>
<hr  />

<p>
This is the home page for Xanadu's quantum machine learning
journal club. We will be working through an idiosyncratic syllbaus on
classical ML, in the hopes it will contribute to our expertise in QML,
offer new insights or research approaches, and generally enhance our quality of life.
</p>


<div class="figure">
<p><img src="./img/qml/ML.png" alt="ML image" align="center" width="400px" style="display:inline;margin:-20px;" />
</p>
</div>

<p>
Here is the schedule (to be populated):
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">Date</th>
<th scope="col" class="left">Material</th>
<th scope="col" class="left">Guide</th>
<th scope="col" class="left">Scribe</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left"><a href="#week0">Jan 9</a>, 2024</td>
<td class="left">Admin</td>
<td class="left">David Wakeham (DW)</td>
<td class="left">DW</td>
</tr>

<tr>
<td class="left"><a href="#week1">Jan 16</a>, 2024</td>
<td class="left">ESL: §2.1-2.4</td>
<td class="left">DW</td>
<td class="left">DW</td>
</tr>

<tr>
<td class="left">&#xa0;</td>
<td class="left">(SVM: §1.1)</td>
<td class="left">&#xa0;</td>
<td class="left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
Notes on past meetings are in chronological order below.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> <a id="week0" name="week0"></a> Admin</h2>
<div class="outline-text-2" id="text-1">
<hr  />

<p>
<b>Reading</b>: None.
</p>

<p>
Based on popular accord, we'll be focusing on the classical
statistical approach to ML embodied in <a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf"><i>Elements of Statistical Learning</i></a>, and the
effective field theory paradigm of <a href="https://arxiv.org/pdf/2106.10165.pdf"><i>The Principles of Deep
Learning</i></a>. These are like the out-of-field examiner and the scary
departmental theorist at your viva. These foci will not stop us from being
eclectic, and dipping into resources on <a href="https://arxiv.org/pdf/2104.13478.pdf"><i>Geometric Deep Learning</i></a>,
<a href="https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf"><i>Support Vector Machines</i></a> or <a href="https://www.deeplearningbook.org/"><i>Deep Learning</i></a> proper, as time, interest, and chunking limits permit.
</p>

<p>
Each week we will have designated reading, optional readings (in
parentheses in the schedule), and a guide or guides who have agreed (or been
voluntold) to talk us through the readings. There will
also be a scribe to take notes. Notes will be uploaded here. For any
corrections, email <a href="mailto:7@heptar.ch"><code>&lt;7@heptar.ch&gt;</code></a>.
</p>

<p>
Finally, please be aware that notation will change as we look at
different resources. It seems easier to be clear about this changes,
and locally consistent, than to try and make everything uniform.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> <a id="week1" name="week1"></a> Overview of supervised learning</h2>
<div class="outline-text-2" id="text-2">
<hr  />
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Statistical learning theory in brief</h3>
<div class="outline-text-3" id="text-2-1">
<p>
<b>Reading</b>: <a href="#SVM">SVM</a>: §1.1.
</p>

<p>
In statistical learning theory, our goal is to optimally approximate a
function \(X \to Y\) using a <i>training set</i> of input-output samples,
\[
\mathcal{D} = \{(x_{(1)}, y_{(1)}), \ldots, (x_{(n)}, y_{(n)})\}.
\]
We assume, further, that these are independently generated according to
\[
P(x, y) = P_X(x) P(y|x),
\]
in other words, we choose \(x\) via the marginal distribution
\(\mathbb{P}_X(\cdot)\) and \(y\) via the conditional distribution
\(\mathbb{P}(\cdot|x)\).
This latter distribution accomodates noise and stochasticity in the
function \(f\).
We assume both distributions are unknown.
<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
In particular, we're not doing <i>parameteric</i> estimation.
	   </span>

</p>

<p>
For simplicity, suppose \(Y \subseteq \mathbb{R}\).
A <i>loss function</i> is a map \(L: X \times Y \times \mathbb{R} \to
[0, \infty)\) which measures how well \(f\) interpolates the data point
\((x, y)\), with smaller values of \(L(x, y, f(x))\) as \(f(x)\) gets closer
to \(y\).
The <i>expected loss</i> or <i>risk</i> for a given \(f: X\to Y\), over the joint
distribution \(P(x, y)\), is
\[
\mathcal{R}(f) = \int_{X\times Y} L(x, y, f(x))\, \mathrm{d}P(x, y).
\]
This is the average loss when we select an infinite number of pairs
according to \(P(x, y)\). The <b>empirical risk</b> is the empirical
approximation using our training pairs,
\[
\mathcal{R}_{\mathcal{D}}(f) = \frac{1}{n}\sum_{i=1}^n L(x_{(i)}, y_{(i)}, f(x_{(i)})).
\]
As \(n \to \infty\), the law of large numbers implies that
\(\mathcal{R}_{\mathcal{D}} \to \mathcal{R}\), i.e. the empirical risk
converges in probability to the risk. So for many data points, we
can approximate this expected loss well.
How do we learn the approximating function \(f\) based on these ideas?
The minimal risk for a given loss function and distribution is
\[
\mathcal{R}^* = \inf_{f: X \to Y} \mathcal{R}(f).
\]
<i>Learning methods</i> uniquely associate functions to training data,
\(\mathcal{D} \mapsto f_{\mathcal{D}}\). The hope is that they
approximate the minimal risk, \(\mathcal{R}(f_{\mathcal{D}}) \approx
\mathcal{R}^*\), at least as our training set gets asymptotically
large. More formally, we say a learning method is <b>universally
consistent</b> if
\[
\lim_{n\to \infty} \mathcal{R}(f_{\mathcal{D}_n}) = \mathcal{R}^*,
\]
i.e. the function we learn minimizes risk.
</p>

<p>
In some ways, it's surprising such methods can exist for arbitrary
distributions \(P\).
In another way, it isn't, because we're only requiring asymptotic
consistency, so we can gather enough data to map out \(P(x, y)\) to
arbitrary accuracy.
And indeed, the <i>no free lunch (NFL) theorem</i> states that, for any
given speed of convergence (specified by a decreasing sequence), and
any universally consistent learning method, there is some probability
distribution which cannot be learned that quickly.
</p>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Linear regression</h3>
<div class="outline-text-3" id="text-2-2">
<p>
<b>Reading</b>: <a href="#ESL">ESL</a>: §2.3.1.
</p>

<p>
This is quite high concept, so let's dive into some specific methods
for prediction, which also give us a sense of how statisticians think.
One of the important tools in statistics is <i>linear models</i> aka
<i>linear regression</i>.
In the case our domain \(X = \mathbb{R}^p\), and \(Y = \mathbb{R}\), and
we have reason to the inputs and outputs are linearly related, we can
write
\[
f(x) = \hat{\beta}_0 + \sum_{j=1}^p x_j\hat{\beta}_j = x^T \hat{\beta}
\]
where \(x_j\) is the \(j\)-th component of the vector \(x\), and in the
second equation, we have padded out \(x\) with a \(1\) in the zeroth
component.
The coefficients \(\hat{\beta}\) form a vector of \(p+1\) parameters (with
the hat reminding us they are to be estimated), but we
can extend this to a matrix for \(Y = \mathbb{R}^k\).
For the moment, let's keep \(k=1\), and absorb the padding into \(p\).
</p>

<p>
As before, we would like to pick the best \(\beta\) for our data using a
loss function. A natural and in some sense optimal choice for linear
models is called the <i>residual sum of squares</i>, where the loss
function is just squared distance:
\[
L(x, y, f(x)) = |y - f(x)|^2 = |y - x^T \beta|^2.
\]
Let \(\mathbf{y} = (y_{(i)})^T\) be a column vector of \(n\) training
outputs, and \(\mathbf{X} = (x_{(i)}^T)\) an \(n\times p\) matrix of
training inputs.
The empirical risk can then be written
\[
\mathcal{R}_{\mathcal{D}}(\beta) = \frac{1}{n}\sum_{i=1}|y_{(i)} -
x_{(i)}^T\beta|^2 = \frac{1}{n}(\mathbf{y} - \mathbf{X}\beta)^T
(\mathbf{y} - \mathbf{X}\beta).
\]
We can solve this for the optimal \(\beta\), simply by differentiating
with respect to \(\beta\):
\[
\partial_\beta \mathcal{\mathcal{R}_{\mathcal{D}}(\beta)}
= \frac{1}{n}\left[\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}
\right) + \left(\mathbf{X}\beta -\mathbf{y} \right)^T
\mathbf{X}\right] = \frac{2}{n}\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}\right),
\]
where the last equality follows from the fact that a scalar is its own transpose.
More carefully, we can differentate either component-wise, or with
respect to both \(\beta\) and \(\beta^T\) as formal variables. Assuming
\(\mathbf{X}^T\mathbf{X}\) has an inverse, we can set this to zero and
solve for \(\beta\):
\[
\mathbf{X}^T\left(\mathbf{X}\beta -\mathbf{y}\right) =
\mathbf{X}^T\mathbf{X}\beta -\mathbf{X}^T\mathbf{y} = 0 \quad
\Longrightarrow \quad \hat{\beta} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.
\]
Again, the hat tells us we will use this as our estimate.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> References</h2>
<div class="outline-text-2" id="text-3">
<hr  />
<ol class="org-ol">
<li><a id="ESL" name="ESL"></a> <a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf"><i>Elements of Statistical Learning</i></a> (2008), Hastie, 
Tibshirani and Friedman. [ESL]
</li>
<li><a id="PDL" name="PDL"></a> <a href="https://arxiv.org/pdf/2106.10165.pdf"><i>The Principles of Deep Learning</i></a> (2021), Roberts and 
Yaida. [PDL]
</li>
<li><a id="GDL" name="GDL"></a> <a href="https://arxiv.org/pdf/2104.13478.pdf"><i>Geometric Deep Learning</i></a> (2021), Bronstein, Bruna,
Cohen, and Veličković. [GDL]
</li>
<li><a id="SVM" name="SVM"></a> <a href="https://pzs.dstu.dp.ua/DataMining/svm/bibl/Support_Vector.pdf"><i>Support Vector Machines</i></a> (2008), Steinwart and Christmann. [SVM]
</li>
<li><a id="DLB" name="DLB"></a> <a href="https://www.deeplearningbook.org/"><i>Deep Learning</i></a> (2015), Aaron Courville, Ian Goodfellow,
and Yoshua Bengio. [DLB]
</li>
</ol>
</div>
</div>
</div>
</body>
</html>