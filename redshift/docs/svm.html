<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title></title>
<!-- 2024-08-06 Tue 00:42 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="style.css" ><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1><b>Transformers as SVMs</b></h1>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title"></h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Introduction</a></li>
<li><a href="#sec-2">2. Attention heads</a></li>
<li><a href="#sec-3">3. Margins and paths</a>
<ul>
<li><a href="#sec-3-1">3.1. Formulating SVMs</a></li>
<li><a href="#sec-3-2">3.2. Connecting to the loss landscape</a></li>
</ul>
</li>
<li><a href="#sec-4">4. Gradient descent</a>
<ul>
<li><a href="#sec-4-1">4.1. Global convergence</a></li>
<li><a href="#sec-4-2">4.2. Local convergence</a></li>
</ul>
</li>
<li><a href="#sec-5">5. Multi-token composition*</a>
<ul>
<li><a href="#sec-5-1">5.1. A nonlinear expansion</a></li>
<li><a href="#sec-5-2">5.2. The general SVM</a></li>
</ul>
</li>
<li><a href="#sec-6">6. Some quantum homework</a></li>
<li><a href="#sec-7">7. References</a></li>
</ul>
</div>
</div>
<div class="right">
<p>
<i>Wherein linear attention is found to be an SVM, but nonlinear
attention is not.</i> 
</p>

</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<hr  />
<ul class="org-ul">
<li>Understanding how transformers work is kind of important. But
because they're complex, we don't know much!
</li>
<li>We could go <i>data first</i>, and try to understand the statistics of the
data they work well on. [<a href="#SVM">1</a>] goes <i>architecture first</i>,
and characterizes the optimization landscape in a data-independent
way. In its own words, it answers the questions:
<ul class="org-ul">
<li><i>Can we characterize the optimization landscape and implicit bias
of transformers?</i>
</li>
<li><i>How does the attention layer select and compose tokens when trained with GD?</i>
</li>
</ul>
</li>
<li>The answer to "can we" is a relatively comprehensive "yes", and to
"how" is "a single layer transformer trained by gradient descent
(GD) solves a max-margin SVM problem which isolates locally optimal tokens".
</li>
<li>For intuition, this is similar to the fact that logistic loss, trained
with GD, converges to a max-margin SVM on separable data
sets. Instead of logistic loss, we have a similar looking softmax.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Attention heads</h2>
<div class="outline-text-2" id="text-2">
<hr  />
<ul class="org-ul">
<li>Maria gave us a nice intro to transformers earlier this year, so
I'll just briefly recapitulate what we need.
</li>
<li>Imagine that our data
lives in some \(d\)-dimensional feature
space (for instance, with an autoencoder like
<code>word2vec</code>). For two sequences of \(T\) data
points, \(\mathbf{X}, \mathbf{Z} \in \mathbb{R}^{T \times d}\), we
will be interested in how \(\mathbf{Z}\) "attends to"
\(\mathbf{X}\). Our setup is called an <i>attention head</i>.
</li>
<li>We model this with two new embeddings, designed to capture attentionally relevant features.
More precisely, define <i>key</i> and <i>query</i> matrices \(\mathbf{K},
  \mathbf{Q} \in \mathbb{R}^{d\times m}\), where \(m\) is a
rank constraint whose significance will become clear later.
</li>
<li>Multiplying
\(\mathbf{X}\) by the key matrix gives a sequence of lookup <i>keys</i>
\(\mathbf{X}\mathbf{K} = \mathbf{k}\), and multiplying \(\mathbf{Z}\)
by the query matrix gives a sequence of <i>queries</i> \(\mathbf{Z}\mathbf{Q}
  = \mathbf{q}\).
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm1.svg" alt="cat/spider image" align="center" width="160px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>The "raw" attention that query \(q_i\) pays to key \(k_j\) is \(q_i
  \cdot k_j\).
We collect all these dot products in a matrix
\(\mathbf{q}\mathbf{k}^\top\). We normalize and filter out the most
relevant keys by applying the softmax function \(\mathtt{S}\) to each query:
\[
  \mathtt{S}(q_i\mathbf{k}^\top) = \left[\frac{e^{q_i\cdot k_j}}{\sum_j e^{{q_i\cdot k_j}}}\right].
  \]
When applied to the matrix, we implicitly mean query-wise.
This gives a smoothed version of of the argmax function, as we
picture below:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm2.svg" alt="cat/spider image" align="center" width="480px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Each row of the softmax matrix is a probability distribution over
keys. Thus, multiplying by \(\mathbf{X}\) returns a weighted
combination of keys for each query. Usually this is embedded once
more via a <i>value matrix</i>, and run through a multi-layer perceptron
(MLP) before being collapsed into a scalar.
</li>
<li>We're going to make some simplifications at this point. First, we'll
consider the dynamics of a single query \(z\).
Second, we'll collapse the details of the value matrix and the MLP
into a <i>prediction head</i> \(h: \mathbb{R}^d \to \mathbb{R}\), applied
to the output vector,
\[
  h(a), \quad a = \mathtt{S}(z\mathbf{Q}\mathbf{K}^\top \mathbf{X}^\top)\mathbf{X}.
  \]
In words, we get a softmax attention-weighted combination of
vectors from \(\mathbf{X}\), and map this to a number.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm3.svg" alt="cat/spider image" align="center" width="450px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>I'll mostly focus on a <i>linear</i> prediction head, which simply takes
the dot product of the output vector \(s \in \mathbb{R}^d\) with some value
vector \(v \in \mathbb{R}^d\), \(h(a) = a \cdot v\).
</li>
<li>This number can be positive or negative, a fact we can use to turn
our prediction head into a binary classifier. Formally, consider a
set of \(n\) data points,
\((\mathbf{X}_i, z_i, Y_i)_{i=1}^n\), where \(Y_i = \pm 1\).
The empirical risk for \(\mathbf{K}\) and \(\mathbf{Q}\) is
\[
  \mathcal{L}(\mathbf{K}, \mathbf{Q}) = \frac{1}{n}\sum_{i=1}^n \ell
  \left(Y_i \cdot h(a_i)\right), \quad a_i = \mathtt{S}(z_i\mathbf{Q}\mathbf{K}^\top
  \mathbf{X}_i^\top)\mathbf{X}_i, \tag{1}\label{loss-QK}
  \]
for some decreasing loss function \(\ell\).
Since \(\mathbf{K}\) and \(\mathbf{Q}\) always appear in the
combination \(\mathbf{W} = \mathbf{Q}\mathbf{K}^\top\), we can train
this <i>weight matrix</i> instead:
\[
  \mathcal{L}(\mathbf{W}) = \frac{1}{n}\sum_{i=1}^n \ell
  \left(Y_i \cdot
  h(a_i)\right), \quad a_i=\mathtt{S}(z_i\mathbf{W}\mathbf{X}^\top)\mathbf{X}_i. \tag{2}\label{loss-W}
  \]
These aren't equivalent because factorization forces \(\mathbf{W}\) to
have rank \(m\).
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Margins and paths</h2>
<div class="outline-text-2" id="text-3">
<hr  />
</div>
<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Formulating SVMs</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>Our goal will be to characterize the attention head in terms of an
SVM problem for separating tokens. The goal: to
max-margin sort the best from the rest.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm5.svg" alt="cat/spider image" align="center" width="500px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>To formulate this SVM, consider a data point \((\mathbf{X}_i, z_i,
  Y_i)\). We define the <i>score</i> of each token in the sequence \(x_{it} \in
  \mathbf{X}_i\) as
\[
  \gamma_{it} = Y_i h(x_{it}) = Y_i (x_{it} \cdot  v). \tag{3} \label{score}
  \]
 An optimal token maximizes this, \(\mathtt{opt}_i \in
  \text{argmax}_t \gamma_{it}\).
</li>
<li>For the \(\mathbf{W}\)-parameterized risk \((\ref{loss-W})\), we can
associate the following SVM:
\[
  \mathbf{W}^*_2 = \text{argmin}_{\mathbf{W}} \Vert \mathbf{W} \Vert_2
  \quad \text{such that} \quad z_i
  \mathbf{W}(x_{i\mathtt{opt}_i}-x_{it})^\top \geq 1 \tag{4} \label{2-SVM}
  \]
for all \(t \neq \mathtt{opt}_i\), where \(\Vert \cdot \Vert_2\) is the
Frobenius or Schatten 2-norm.
</li>
<li>Feasibility of this SVM problem requires that the optimal tokens are
linearly separable from the others. This is always possible for
if we embed our data in suitably high dimension \(d\). In fact, \(d
  \geq \max\{T-1, n\}\) works.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm4.svg" alt="cat/spider image" align="center" width="500px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Since softmax involves a smooth convex combination of tokens,
the only way to get the one-hot vector corresponding to a single
token is to have infinite norm:
\[
  \Vert \mathbf{W} \Vert_2 \to \infty.
  \]
So \(\hat{\mathbf{W}}\) converges to \(\hat{\mathbf{W}}_2^*\) as the
norms blow up.
</li>
<li>Analogously, for the \(\mathbf{K},\mathbf{Q}\) factorization we can
minimize the sum
\[
  \frac{1}{2}\left(\Vert \mathbf{K}\Vert_2^2 + \Vert
  \mathbf{Q}\Vert_2^2\right),
  \]
subject to the hard margin constraint. As it <a href="https://mathoverflow.net/questions/297953/nuclear-norm-as-minimum-of-frobenius-norm-product">turns out</a>,
\[
  \min \frac{1}{2}\left(\Vert \mathbf{K}\Vert_2^2 + \Vert
  \mathbf{Q}\Vert_2^2\right) = \min \Vert
  \mathbf{Q}\mathbf{K}^\top\Vert_1 = \min \Vert \mathbf{W}\Vert_1,
  \]
where \(\Vert \cdot \Vert_1\) is the nuclear, trace, or Schatten
1-norm. In pictures:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm6.svg" alt="cat/spider image" align="center" width="320px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>This holds when we constrain \(\mathbf{Q}\), \(\mathbf{K}\),
hence \(\mathbf{W}\) to have rank \(\leq m\). We formulate
\[
  \mathbf{W}^*_1 \in \mathcal{W}_m^*=\underset{\text{rank}(\mathbf{W})\leq m}{\text{argmin}} \Vert \mathbf{W} \Vert_1
  \quad \text{such that} \quad z_i
  \mathbf{W}(x_{i\mathtt{opt}_i}-x_{it})^\top \geq 1. \tag{5} \label{1-SVM}
  \]
In this case, there is a set of solutions \(\mathcal{W}_m^*\) due to the
rank constraint.
</li>
<li>The rank constraint is nonconvex if \(m < d\). However, if the
solution of \((\ref{1-SVM})\) has rank \(\leq m\), then it is recovered
by solving the SVM with rank \(m\). Furthermore, the rank of solutions
is bounded by the number of training points, since
\[
  \mathbf{W}^*_{1, 2} \in \mathtt{span}(z_i) \quad \Longrightarrow
  \quad \mathtt{rank}(\mathbf{W}^*_{1, 2}) \leq n.
  \]
This arises from the usual expansion in training examples.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Connecting to the loss landscape</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>So, now we have formulated these SVM problems. The hope is that
training the transformer solves them!
</li>
<li>The first question is whether the loss function picks out the optimal
tokens selected by the SVM. It's easy to see by convexity that, for
any set of optimal indices \((\mathtt{opt}_i)_{i=1}^n\), the risk is
lower bounded by their average loss:
\[
  \mathcal{L} = \frac{1}{n}\sum_{i=1}^n \ell(Y_i\cdot h(a_i)) \geq \frac{1}{n}\sum_{i=1}^n \ell(\gamma_{i
  \mathtt{opt}_i}) = \mathcal{L}^*.
  \]
Suppose the SVM is feasible, with some separating
\(\mathbf{W}_\mathtt{SVM}\). Then as \(R\)
increases, the norm of \(\mathbf{W}_\mathtt{SVM}\) increases, so
\[
  \lim_{R\to \infty} \mathcal{L}(R\cdot \mathbf{W}_\mathtt{SVM}) = \mathcal{L}^*.
  \]
We have a global minimum that lives "out at infinity".
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm8.svg" alt="cat/spider image" align="center" width="280px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Of course, we want to know if our loss function can actually get
there by GD! This is tricky, so we will warm up with a <i>regularized
path</i> (RP) analysis. This "regularizes" the minimum of the loss
function on a ball of increasing radius:
\[
  \mathbf{W}_R = \underset{\Vert \mathbf{W}\Vert_2 \leq
  R}{\text{argmin}} \, \mathcal{L}(\mathbf{W}) ,\qquad
  (\mathbf{K}_R, \mathbf{Q}_R) = \underset{\Vert \mathbf{Q}\Vert_2^2+\Vert \mathbf{K}\Vert_2^2 \leq 2R}{\text{argmin}}\mathcal{L}(\mathbf{K}, \mathbf{Q}). \tag{6} \label{RP}
  \]
The idea is take \(R \to \infty\) and see where these RP minima go.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm7.svg" alt="cat/spider image" align="center" width="250px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Notice that, since we are not minimizing the \(\mathbf{K},
  \mathbf{Q}\) norms, but the loss function
\(\mathcal{L}(\mathbf{K},\mathbf{Q})\), we cannot simplify to the
\(1\)-norm. Nevertheless, the factorized problem has an <i>implicit
\(1\)-norm bias</i>! The \(\mathbf{W}\) formulation has a \(2\)-norm bias.
Formally:
\[
  \lim_{R\to\infty}\frac{\mathbf{W}_R}{R} =
  \frac{\mathbf{W}^*_2}{\Vert \mathbf{W}^*_2\Vert_2},
  \quad \lim_{R\to\infty}\mathtt{dist}\left(\frac{\mathbf{Q}_R
  \mathbf{K}^\top_R}{R}, \frac{\mathcal{W}^*_m}{\min \mathcal{L}(\mathcal{W}^*_m)}\right) = 0,
  \]
where \(\mathtt{dist}\) is Euclidean distance from a point to a set.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Gradient descent</h2>
<div class="outline-text-2" id="text-4">
<hr  />
<ul class="org-ul">
<li>So far, we've ignored how the weight matrix (or its factors) are
trained. We focus on vanilla GD, and for learning rate \(\eta > 0\), \(k
  \in \mathbb{N}\), define
\[
  \mathbf{W}(k + 1) = \mathbf{W}(k) - \eta \nabla \mathcal{L}[\mathbf{W}(k)],
  \]
with some initial \(\mathbf{W}(0) \in \mathbb{R}^{d\times d}\), and
similarly for \(\mathbf{Q}, \mathbf{K}\).
</li>
</ul>
</div>
<div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> Global convergence</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>We start by enumerating some sufficient conditions for global
convergence of GD.
</li>
<li>Suppose that the optimal token index
\(\mathtt{opt}_i\) is unique, and moreoever, non-optimal tokens \(t, t'
  \neq \mathtt{opt}_i\) in each sequence are (B.1) support vectors,
\[  z_i\mathbf{W}^* (x_{i\mathtt{opt}_i} - x_{it})^\top = 1,
  \]
and (B.2) have nearly equal scores, \(|\gamma_{it} - \gamma_{it'}| \leq
  \delta\) for some dataset-dependent \(\delta\). Then you can show
global convergence! More formally, you can prove that:
<ol class="org-ol">
<li>there are no local stationary points, i.e. no \(\mathbf{W}\) such
that \(\nabla\mathcal{L}(\mathbf{W}) = 0\);
</li>
<li>assuming (B.1), if \(\eta\) is sufficiently small, the norm of
\(\mathbf{W}(k)\) diverges:
\[
      \lim_{k\to\infty}\Vert
     \mathbf{W}(k)\Vert_2 = \infty,
     \]
which we require for converge to a one-hot output vector; and
</li>
<li>assuming also (B.2), then GD directionally converges to the
solution of \((\ref{2-SVM})\):
\[
      \lim_{k\to\infty}\frac{\mathbf{W}(k)}{\Vert\mathbf{W}(k)\Vert_2}
     = \hat{\mathbf{W}}^*_2.
     \]
</li>
</ol>
</li>
<li>Let's examine these assumptions. (B.1) should generically result from
overparametrization, since as \(d\) increases, there is enough
hyperdimensional wiggle room to pass a plane through each
non-optimal token:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm10.svg" alt="cat/spider image" align="center" width="500px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>The authors conjecture that \(d \gtrsim (T+n)\log(T+n)\) is enough,
and provide some numerical evidence. Green bars indicate the
proportion of \(500\) cases in which (B.1) is satisfied for random data:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm9.svg" alt="cat/spider image" align="center" width="850px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Assumption (B.2), in contrast, is non-generic, since even as \(d\)
increases, the SVM objective does not drive non-optimal tokens to have
similar scores. But if (B.2) fails, local minima can come into being
which trap GD, as I'll discuss shortly.
</li>
<li>Nonetheless, with a good
initialization we can avoid these bad minima. Define
\[
  \mathcal{C}_{\mu, R} = \{\mathbf{W} : \Vert \mathbf{W}\Vert_2 \geq R,\,\,
   z_i\mathbf{W}^* (x_{i\mathtt{opt}_i} - x_{it})^\top \geq \mu
  \text{ for all } t \neq \mathtt{opt}_i\}.
  \]
These are weight matrices outside a ball of radius \(R\) that
separate optimal from non-optimal tokens with margin \(\mu\).
You can show that for any \(\mu > 0\), for sufficiently large \(R\) this
set has no local stationary points.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm11.svg" alt="cat/spider image" align="center" width="350px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Moreover, if GD arrives there, and updates keep it there, then it
converges:
   \[
      \lim_{k\to\infty}\frac{\mathbf{W}(k)}{\Vert\mathbf{W}(k)\Vert_2}
     = \hat{\mathbf{W}}^*_2.
     \]
This is nice and all, but it raises the thorny question of what
local minima look like and how to avoid them. This is what we turn
to next!
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> Local convergence</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Since the weight matrix heads off infinity under mild conditions, we
need to worry about <i>locally optimal directions</i>. Let's define these
now.
</li>
<li>Consider a set of suboptimal indices \(\boldsymbol{\alpha} =
  (\alpha_i)^n_{i=1}\), one for each data point. Suppose that
\((\ref{2-SVM})\) is feasible with \(\boldsymbol{\alpha}\) replacing
\(\mathtt{opt}\), with minimum \(2\)-norm solution
\(\mathbf{W}^{\boldsymbol{\alpha}}_2\). For each \(i\), let
\(\mathcal{T}_i \subseteq [T]\) denote the set of support indices satisfying
\[
  z_i\mathbf{W}^* (x_{i\alpha_i} - x_{it})^\top = 1.
  \]
If the \(\alpha_i\) have better scores than the support indices,
\(\gamma_{i\alpha_i} > \gamma_{it}\), then
\(\mathbf{W}^{\boldsymbol{\alpha}}_2\) is a <i>locally optimal
direction</i>. GD only "looks at" scores of
support indices!
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm12.svg" alt="cat/spider image" align="center" width="350px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>We saw above that with a good initialization, GD can end in the
global minimum. The same proof shows that, with a bad
initialization, it ends in a local minimum! The construction
involves an analogous set \(\mathcal{C}_{\mu,
  R}(\mathbf{W}^{\boldsymbol{\alpha}}_2)\):
\[
  \mathcal{C}_{\mu,
  R}(\mathbf{W}^{\boldsymbol{\alpha}}_2) = \{\mathbf{W} : \Vert
  \mathbf{W}\Vert_2 \geq R, \,\,
   \mbox{Tr}[\hat{\mathbf{W}}^\top\hat{\mathbf{W}}^{\boldsymbol{\alpha}}_2]
  \geq 1-\mu\}.
  \]
</li>
<li>We can see this convergence numerically. Below are the correlation
coefficients for the GD weights and locally optimal solution, when
appropriately initialized.
We see the expected \(2\)-norm and \(1\)-norm bias:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm13.svg" alt="cat/spider image" align="center" width="600px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>So, is there a way to ensure the absence of local minima, other than
the (unrealistic) assumption of nearly equal scores? There is! We
simply find ways to prevent any \(\boldsymbol{\alpha}\) from giving a locally
optimal direction.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm15.svg" alt="cat/spider image" align="center" width="180px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Recall that we need \(\alpha_i\) to have a better
score than its support indices, for all \(i\). However, \(\alpha_i\)
never has a better score than \(\mathtt{opt}_i\). So we simply require
\(\mathtt{opt}_i\) to be support for some \(i\). This lets GD "sense"
the suboptimality.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm14.svg" alt="cat/spider image" align="center" width="350px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>This condition is hard to check due to the combinatorial explosion
of \(\boldsymbol{\alpha}\). But there is a stronger hypothesis:
the optimal indices are support for <i>every</i> \(i\). This would be much
easier to check for a given probabilistic model of data!
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Multi-token composition*</h2>
<div class="outline-text-2" id="text-5">
<hr  />
<ul class="org-ul">
<li>We've covered the main result: a single-layer transformer with
linear prediction head can be characterized in terms of SVM
max-margin problems for selecting optimally attended-to tokens. When
non-optimal tokens are support for optimal tokens, and vice versa,
then GD converges to the global minimum.
</li>
</ul>
</div>
<div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> A nonlinear expansion</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>The authors note that, realistically, for multiple layers or
nonconvex heads \(h(\cdot)\), attention will select multiple
tokens per sequence. This raises the question:
<ul class="org-ul">
<li><i>What is the implicit bias and the form of \(\mathbf{W}(k)\) when
the GD solution is composed by multiple tokens?</i>
</li>
</ul>
</li>
<li>By "multiple tokens", we mean some but not all; since this has zero
entries, it implies that \(\Vert \mathbf{W}(k)\Vert \to
  \infty\). Provide the convergence is not pathological, the authors
expect expect the GD solution can be written
\[
  \mathbf{W}(k) = \Vert \mathbf{W}(k) \Vert \cdot \hat{\mathbf{W}}^* +
  \mathbf{W}^\text{fin} + o(k), \tag{7} \label{decomp}
  \]
where \(\hat{\mathbf{W}}^*\) is the direction of convergence, and
\(\mathbf{W}^\text{fin}\) is a finite matrix.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm17.svg" alt="cat/spider image" align="center" width="320px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>They don't explain their intuition, but this is basically a Laurent
expansion at infinity. If \(x = 1/k\), we schematically expect
something like
\[
  \mathbf{W}(x) = f\left(\frac{1}{x}\right)\mathbf{W}^* +
  \mathbf{W}^\text{fin} + o(x),
  \]
where \(f = O(1/x)\). If \(\mathbf{W}\) blows up polynomially in \(k\),
then for some power \(m\),
\[
  \lim_{x\to 0} x^{m} \mathbf{W}(x) =\lim_{k\to \infty}
  \frac{1}{k^{m}} \mathbf{W}(k) = \Vert \mathbf{W}^* \Vert \hat{\mathbf{W}}^*,
  \]
We'll see another way to view the decomposition in a moment, but it
would be cool to explore this expansion at infinity!
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> The general SVM</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>We will try to formulate a corresponding SVM problem. Consider the
final softmax probability distribution
\[
  s_i^* = \mathtt{S}(z\mathbf{W}^* \mathbf{X}^\top)
  \]
with <i>selected</i> entries \(s_{it}^* > 0\) for \(t \in \mathcal{O}_i
  \subseteq [T]\), and <i>masked</i> entries \(s_{it}^* = 0\) for \(t \in
  \bar{\mathcal{O}}_i\).
</li>
<li>The decomposition of \(\mathbf{W}(k)\) has a simple interepretation:
the directional part selects/masks tokens, and the finite component
assigns probabilities.
Concretely,
\[
  s_{it}^* = Ce^{z_i \mathbf{W}^{*}x_{it}^{\top}} e = C e^{z_i
  \mathbf{W}^{\text{fin}}x_{it}^{\top}}e^{\Vert\mathbf{W}^*\Vert z_i \hat{\mathbf{W}}^*x_{it}^{\top}}.
  \]
The second factor either goes to zero or blows up, depending on the
sign of \(z_i \hat{\mathbf{W}}^*x_{it}^{\top}\), which achieves the
masking or selection.
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm18.svg" alt="cat/spider image" align="center" width="220px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>If \(t, t' \in \mathcal{O}_i\) are both selected, it's natural to take
their infinite parts to be equal,
\[
  e^{\Vert\mathbf{W}\Vert z_i \hat{\mathbf{W}}x_{it}^{\top}} =
  e^{\Vert\mathbf{W}\Vert z_i \hat{\mathbf{W}}^*x_{it'}^{\top}}
  \quad \Longrightarrow \quad  z_i \hat{\mathbf{W}}^*(x_{it}-
  x_{it'})^{\top} = 0.
  \]
This implies that the finite part obeys
\[
  \frac{e^{z_i \mathbf{W}^{\text{fin}}x_{it}^{\top}}}{e^{z_i
  \mathbf{W}^{\text{fin}}x_{it'}^{\top}}} =
  \frac{s_{it}^*}{s_{it'}^*} \quad \Longrightarrow \quad 
  z_i \mathbf{W}^{\text{fin}}(x_{it} - x_{it'})^{\top} =
  \log\left(\frac{s_{it}^*}{s_{it'}^*}\right).
  \]
</li>
<li>Finally, if \(t \in \mathcal{O}_i\) is selected and \(t'' \in
  \bar{\mathcal{O}_i}\) is masked, then the ratio should diverge, with
the directional component dominating:
\[
  \frac{s^*_{it}}{s^*_{it''}} \sim \frac{e^{\Vert \mathbf{W}\Vert
  z_i \hat{\mathbf{W}}^{*}x_{it}^{\top}}}{e^{\Vert \mathbf{W}\Vert z_i
  \hat{\mathbf{W}}^{*}x_{it''}^{\top}}} = e^{\Vert \mathbf{W}\Vert
  z_i \hat{\mathbf{W}}^{*}(x_{it} - x_{it''})^{\top}} \to \infty.
  \]
This suggests a max-margin constraint of the form \(z_i
  \hat{\mathbf{W}}^{*}(x_{it} - x_{it''})^{\top} \geq 1\).
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm19.svg" alt="cat/spider image" align="center" width="230px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>Thus, we have an SVM problem that corresponds to multi-token composition:
<ul class="org-ul">
<li>\(\mathbf{W}^*_\Diamond = \text{argmin}_{\mathbf{W}} \Vert \mathbf{W}\Vert_\Diamond\), for a norm \(\Diamond\), such that:
</li>
<li>\(z_i \mathbf{W}_\Diamond^*(x_{it}-x_{it'})^\top =0, \quad \text{ for all }t, t' \in\mathcal{O}_i\);
</li>
<li>\(z_i \mathbf{W}_\Diamond^*(x_{it}-x_{it''})^\top \geq 1, \quad \text{ for all }t
    \in\mathcal{O}_i, t'' \in\bar{\mathcal{O}}_i\).
</li>
</ul>
</li>
<li>They don't prove much about this formulation, partly because it
seems to be subject of an upcoming paper. But they do some nice
numerical experiments with a nonconvex (\(\mathtt{ReLU}\)) head and
random data:
</li>
</ul>

<div class="figure">
<p><img src="./svm-img/svm16.svg" alt="cat/spider image" align="center" width="950px" style="display:inline;margin:-20px;" />
</p>
</div>
<ul class="org-ul">
<li>The plots show the correlation coefficient for three baselines. The
first two are SVM problems, namely the single token problem
\((\ref{2-SVM})\), and the solution of the multi-token SVM with
\(2\)-norm penalty. Unsurprisingly, the latter does better.
</li>
<li>However, better than either of these is an analytical solution based
on \((\ref{decomp})\), which calculates the finite part
\(\mathbf{W}^{\text{fin}}\) from softmax probabilities, then tunes the
coefficient \(\gamma\) of the direction in
\[
  \mathbf{W}^\text{fin} + \gamma \hat{\mathbf{W}}^*
  \]
to maximize correlation with the GD solution.
</li>
<li>This suggests that
nonlinear attention is <i>not</i> best viewed as an SVM, but rather, in
terms of
its effective dynamics at infinity. Sounds like a cool physics problem!
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Some quantum homework</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>Does any of this bear on QML? In the linear setting, where attention
can be provably related to an SVM, it feels highly relevant!
</li>
<li>This leads to our first homework question:
<ul class="org-ul">
<li><i>Can we (naturally) embed linear attention in a quantum model?</i>
</li>
</ul>
</li>
<li>The paper also answers a question I considered a few weeks back,
regarding <i>sparsity</i>. In the linear case, only one token is chosen
per input sequence. We may be able to sparsify training by,
e.g. computing a single term in the loss function, and then use
Grover to find this term quickly.
</li>
<li>This leads to the second question:
<ul class="org-ul">
<li><i>Can we sparsify training in such a way that Grover applies?</i>
</li>
</ul>
</li>
<li>The last question is not really quantum, but it is physics:
<ul class="org-ul">
<li><i>For nonlinear attention, what determines the effective dynamics of GD at infinity?</i>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> References</h2>
<div class="outline-text-2" id="text-7">
<hr  />
<ol class="org-ol">
<li><a href="https://arxiv.org/abs/2308.16898">"Transformers as Support Vector Machines"</a>
(2018). Davoud Ataee Tarzanagh, Yingcong Li, Christos
Thrampoulidis, Samet Oymak. <a id="SVM" name="SVM"></a>
</li>
</ol>
</div>
</div>
</div>
</body>
</html>