---
Layout: post
mathjax: true
comments: true
subtitle: "Six Impossible Things 1"
title: ""
categories: [6IT]
date:  2024-01-05
---

**January 5, 2023.** *In the first issue of 6IT: how to supervise children; unreliably clairvoyant
demons; why the sky is blue (and why it isn't); a Poisson process for
primes; simulating fireflies;*

<h1>Six Impossible Things (Issue 1)</h1>

<div style="background-color: #212433 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
“I daresay you haven't had much practice,” said the Queen. “When I was
your age, I always did it for half-an-hour a day. Why, sometimes I've
believed as many as six impossible things before breakfast.”<br>

<div style="text-align: right">— Lewis Carroll, <i>Through the Looking Glass</i>
</div>
</div>

Inspired by one of my heroes, [John Baez](https://math.ucr.edu/home/baez/TWF.html), I'm
planning to blog about "finds" in the hard sciences on a semi-regular basis. These
may be papers, favourite theorems, new discoveries, or shower thoughts; I'm hoping to
be brief and nerdily enthusiastic rather than thorough and technically
deep. Let's see how it goes!

## Contents <a id="tbc" name="tbc"></a>

1. <a href="#sec-1"><i>The obstructed supervision theorem</i></a>
2. <a href="#sec-2"><i>Zermelo's demon and the axiom of choice</i></a>
3. <a href="#sec-3"><i>Why the sky is blue</i></a>
4. <a href="#sec-4"><i>A game of primes</i></a>
5. <a href="#sec-5"><i>The firefly effect</i></a>

## 1. <a href="#tbc">The obstructed supervision theorem</a><a id="sec-1" name="sec-1"></a>

Suppose you have to guard an art gallery with $n$ straight walls. How many guards do you need
to ensure the artworks are visible at all times? There is a beautiful
proof, due to Steve Fisk and presented
[Proofs from THE BOOK](https://link.springer.com/book/10.1007/978-3-662-57265-8),<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   This book collects demonstrations of which God themself might
	   approve. Whether an omniscient being needs proofs is another question!
	   </span>
that $\lfloor n/3\rfloor$ guards are sufficent, and examples show
this number is in general necessary. We'll start with the
examples. Take $m$ triangles and connect them with a
corridor, forming a gallery of $n=3m$ walls. For $m = 3$, this gives:

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards1.svg" width="330"/>
	</div>
	</figure>

Each triangle requires a guard to see the recessed tip, but one of
these guards can survey the whole corridor from the bottom left.
Thus, $m = n/3$ guards are needed. For $n$ not a multiple of $3$, we
can simply remove one or two walls from the above construction.
Now let's turn to Fisk's argument
for sufficiency.

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Art Gallery Theorem.</span> For a polygonal gallery of $n$ walls, $\lfloor
n/3 \rfloor$ guards are sufficient.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
   <i>Proof.</i>$\,$ (Summary: triangulate, 3-color, and place
   guards at one color. Click to expand.)
  </summary>
  <p>
Consider the graph formed by the $n$ walls of the gallery, with
 walls as edges and the points they meet as vertices. Triangulate this graph,
 i.e. introduce $n - 2$ new rectilinear edges that split the interior
 into triangles. We will show that we can color the vertices of this
 graph with three colors so that no adjacent vertex has the same
 color.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards4.svg" width="300"/>
	</div>
	</figure>

Note that, since there are no internal vertices, we can
 assign $0$ and $1$ in an alternating fashion to triangular faces.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
   If we could not assign bits this way, it would mean an odd number
 of triangular faces meet at a point. This is only possible with
 internal vertices.
	   </span> We color
 the faces assigned $0$ with three colors (G, R, B) in clockwise order, and
 faces assigned $1$ with the colors in counterclockwise order, with
 the first face determining the coloring of the second face, and so
 on. The coloring clearly exists and is unique up to the choice of
 triangulation and ordering on the first face.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards2.svg" width="300"/>
	</div>
	</figure>

Running around the polygon, we have $n$ vertices of three
 colors, the rarest of which is assigned to at most $\lfloor n/3 \rfloor$
 vertices. If it was assigned to more, there would be more than
 $n$ vertices!

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards3.svg" width="300"/>
	</div>
	</figure>

We choose this rare color to be the location of our guards. Since each
triangle has a vertex of this color, each triangle has a guard
watching it, and we are done.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
The required triangulation doesn't
obviously exist for non-convex polygons. To show it does, first we
need the existence of a vertex with acute interior angle, like the
leftmost corner above. Without such a vertex, the polygon could never
close! So, let's draw this vertex:

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards8.svg" width="200"/>
	</div>
	</figure>
	
We can either tringulate by drawing an edge between the two
adjacent vertices, or if a vertex pokes inside that triangle, to the
offending poker. Then we split off the triangle and use induction on
the rest!
</span> $\blacksquare$
  </p>
</details>
</div>

I became interested in the art gallery theorem because of a real world
problem: supervision in out-of-school-hours (OSH) care. Instead of
guards watching over paintings, there are educators watching over
children, which in contrast to paintings are mobile and
unpredictable. The requirement of full visibility makes more sense!
My sister runs an OSH facility with an almost comically elaborate
layout.
The standard art gallery theorem does not apply due to the presence of
*internal polygons*.
Let us these call internal walls or polygons *holes*.
With a little thought, one can extend to the art gallery theorem to
the following:

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Obstructed Supervision Theorem.</span> For a polygonal layout of $n$ walls and $g$
holes, a total of

$$
\left\lfloor \frac{n+2g}{3}\right\rfloor
$$

educators are sufficient to supervise the whole space.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: connect a hole to the outside with two walls.)
  </summary>
  <p>
For each hole, draw an edge from one of its vertices to
a vertex on the outer polygon.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards5.svg" width="400"/>
	</div>
	</figure>

If each such edge is viewed as
<i>two</i> walls, we now have a (non-convex) polygon of $n + 2g$ walls,
and we can apply the original art gallery theorem.
$\blacksquare$
  </p>
</details>
</div>

This was first proved by O'Rourke (see Chapter 5 of his book).
Examples of necessity are much harder to come by.
We can modify our earlier examples by adding internal walls, which we view as
degenerate polygons of $2$ walls.
For instance, the following layout, with $n = 7$ and $g = 1$,
requires $(n + 2g)/3 = 3$ educators:

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/guards6.svg" width="230"/>
	</div>
	</figure>

Adding more slits and recesses, we can extend this to higher values of $n$ and $g$.
But these degenerate  polygons seem like cheating,<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
After all, they can only hide a child of measure zero from a
well-positioned supervisor.
</span>
and if we require genuine polygons, it becomes much harder to saturate the bound; for
general $n$ and $g$, the problem is unsolved. There is a full
characterization of such polygons for a *single* hole (due to Shermer)
that is very fiddly, and gives rise to constructions like the following
($n=40$ walls and $14$ guards):

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/guards7.svg" width="230"/>
	</div>
	</figure>

This would not look out of place in a basilica or medieval fortress.
It is remarkable that a few walls make the task so
much harder; then again, perhaps we shouldn't be surprised that
trying to manage groups of children gives rise to unsolvable
problems.

### Further reading

- [*Proofs from THE BOOK*](https://link.springer.com/book/10.1007/978-3-662-57265-8) (1998), Martin Aigner and Günter Ziegler.
- [*Art Gallery Theorems and Algorithms*](https://www.science.smith.edu/~jorourke/books/ArtGalleryTheorems/Art_Gallery_Full_Book.pdf)
  (1987), Joseph O'Rourke.

## 2. <a href="#tbc">Zermelo's demon and the axiom of choice</a><a id="sec-2" name="sec-2"></a>

A [recent paper](https://arxiv.org/abs/2206.08467) by Baumeler, Dakić, and Del Santo introduced me to a curious
consequence of the axiom of choice (AC).
Recall that the AC states that, for any collection of nonempty sets
$\\{S_i: i\in I\\}$, we can choose a set of representatives
$\\{s_i: i\in I\\}$ where each $s_i \in S_i$, i.e. one element from each set.
Though the AC is very intuitive—we can choose elements from nonempty
sets—it has very counterintuitive consequences, such as the
[Banach-Tarski paradox](https://en.wikipedia.org/wiki/Banach-Tarski_Paradox).

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/aoc1.svg" width="500"/>
	</div>
	</figure>

This splits a perfect sphere into five
wildly complicated pieces, and reassembles them to form *two* spheres of
	   the same size!
In his
[autobiography](https://sistemas.fciencias.unam.mx/~compcuantica/RICHARD%20P.%20FEYNMAN-SURELY%20YOU'RE%20JOKING%20MR.%20FEYNMAN.PDF),
Richard Feynman refutes Banach-Tarski<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   Or he mockingly calls it, "So-and-so's theorem of
	   immeasurable measure".
	   </span> on the grounds that no *physical*
sphere, composed of atoms, can be split up this way.
The atoms prevent the arbitrarily fine divisions needed to make the
pieces non-measurable!
One could view this as an argument against the AC, but I think the
point is subtler: since we cannot carry out the Banach-Tarski
"experiment" in reality, we cannot use physics to reason about the axioms of set
theory. The AC could be true, it could be false; physics probably has
nothing to do with it.

Baumeler, Dakić, and Del Santo disagree.
They argue that the AC may have physical implications, using a
bizarre corollary discovered by Gabay and O’Connor in 2004:

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Infinite Hat Theorem.</span>
Suppose a countable number of prisoners are placed on the
number line, one at each natural number, and independently assigned a
blue or red hat with equal probability. We suppose they face forward,
and can see all hats at higher values of $n$.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/aoc2.svg" width="330"/>
	</div>
	</figure>

If the prisoners meet beforehand, can use the AC, and know their
position on the number line,
they can independently guess the color of their hats so that <i>only a finite number
are wrong</i>.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: prisoners see eventually identical sequences, and use AC to backwards complete.)
  </summary>
  <p>
Consider the set $S = \{\text{R},
\text{B}\}^{\mathbb{N}}$ of infinite sequences of red and blue
hats. Two sequences $s, s' \in S$ are eventually equal if they have
the form
$$
s = s_0 t, \quad s' = s'_0 t
$$
for some finite sequences $s_0, s_0'$ of equal length, and $t \in S$.
We let $s(n)$ denote the $n$th element of sequence $s$.
It's easy to see that being eventually equal is an
equivalence relation, with equivalence classes $S_i$ for some indexing
set $i \in I$.
By the AC, the prisoners can select a representative $s_i$ from each
class.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/aoc3.svg" width="330"/>
	</div>
	</figure>

Once placed on the number line and assigned hats, any two prisoners will see
sequences that are eventually equal, with $t$ the sequence seen by the
prisoner further along (see above).
All prisoners can thus determine the equivalence class $S_i$ of the
actual sequence of hats, choose the corresponding representative
$s_i$, and output the guess $s_i(n)$ for their position $n$.
Since the actual sequence and the guess $s_i$ are eventually equal,
at most finitely many guesses are wrong.
$\blacksquare$
</p>
</details>
</div>

There are already a number of Feynman-like objections we could make
here; for instance, the prisoners need to have an infinitely large memory to store
the representatives $s_i$.
But even if we grant them these powers, what can they *do* with them?
Infinite memory is reminiscent of Maxwell's demon and relatives, so let us
replace the prisoners with *Zermelo's demon*, named after Ernst
Zermelo, originator of the AC.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   Prior to his work on set theory, Zermelo invented a demon to sit around
	   and wait for
	   [Poincaré recurrences](https://en.wikipedia.org/wiki/Poincar%C3%A9_recurrence_theorem).
	   These recurrences reset a system to its initial state, thereby spontaneously reducing entropy and violating
	   the second law of thermodynamics. We suppose Zermelo's demon got
	   bored—a recurrence takes much longer
	   than the age of the universe—and decided, like Zermelo, to turn its
	   efforts to mathematics.
	   </span>

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/aoc4.svg" width="330"/>
	</div>
	</figure>

The demon uses AC and has a set of lucky coins; any time it
flips one, it learns the infinitely long history of prior coin
outcomes (analogous to prisoners looking backwards) and the number of
flips until the coin disintegrates (analogous to a prisoner learning their
number). Like the prisoners, the demon can determine the equivalence
class of $S_i$ of the history of flips, and choose the representative
$s_i$ with the AC. But since *any* sequence of remaining flips gives
rise to the same $s_i$, it's clear that the demon learns nothing about them!

The fact that Zermelo's demon cannot predict future coin flips makes
the AC less physically disturbing. But there is another variant due to
[Hardin and Taylor](https://www.jstor.org/stable/27642415) which shows
the demon *can* look into the future. I follow
[this nice exposition](https://ericmoorhouse.org/handouts/Hardin-Taylor.pdf)
by Eric Moorhouse.

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">The Hardin-Taylor Theorem.</span>
Given an arbitrary function $f:\mathbb{R} \to \mathbb{R}$, Zermelo's demon
can use the behaviour of $f$ on $(-\infty, t)$ to correctly predict the
behaviour on $(-\infty, t')$ for some $t' > t$, except for $t$ in a
countable set.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: the demon uses the prisoner's trick
with functions rather than sequences.)
  </summary>
  <p>
  To prove this, the demon first constructs a
well-ordering<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
A well-ordering is a linear order ($\prec, A)$ such that every nonempty set of $A$ has a least
element. We use the AC to pick the least element,
then its successor, and so on.
</span>
$(\prec, \mathbb{R}^{\mathbb{R}})$ on the set of real functions using the AC. 
Consider the equivalence relation on functions $f:
\mathbb{R}\to \mathbb{R}$ given by
$$
f \sim_t g \quad \Longleftrightarrow \quad f(x) = g(x) \text{ for all } x
< t.
$$
The well-ordering allows the demon to define the representative
$
\hat{f}_t = \min_{\prec} \{g : f \sim_t g\}.
$
The demon then guesses that $\hat{f}_t$ is the extension of $f$.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/aoc5.svg" width="350"/>
	</div>
	</figure>
	
This extrapolation will fail on some set of “unpredictable” times,
$$
U = \{t \in \mathbb{R} : \hat{f}_t \neq \hat{f}_{t'} \text{ for all }
t' > t\}.
$$
These are times for which $\hat{f}_t$ fails to agree with $f$ at all
$t' > t$, or equivalently, $\hat{f}_{t'}$.
Our goal now is to show the set $U$ is countable.
Observe that $U$ cannot have an infinite sequence of times, or
equivalently a sequence of functions,
$$t_1> t_2 > \cdots \quad \Longleftrightarrow \quad
\hat{f}_{t_1} \succ \hat{f}_{t_2} \succ \cdots,
$$
since $\hat{f}_{t_{i}} \sim_{t_{i+1}} f$, but $\hat{f}_{t_{i+1}}$
is defined as the minimal function satisfying this, so we must have $
\hat{f}_{t_{i}} \succ \hat{f}_{t_{i+1}} $. This
infinite descending sequence is impossible, since $U$ has a least
element.
Since $U$ has no infinite descending sequence of times, for any $t \in
U$, the set $U_t = U \cap (t, \infty)$ either has a least
element $\min U_t$, or is empty, in which case $t = \max U$.

<figure>
    <div style="text-align:center; padding: 20px"><img src
    ="/img/posts/aoc6.svg" width="330"/>
	</div>
	</figure>

In the first case, we can find a rational number $\iota(t)$ which is
between $t$ and $\min U_t$; in the second, we just choose some
rational $\iota(t) > t$. The map $\iota: U \to \mathbb{Q}$ is an
injection, and since $\mathbb{Q}$ is countable, $U$ is countable.
$\blacksquare$
  </p>
  </details>
</div>

The demon is clairvoyant on almost all intervals $(-\infty, t)$, but with no
guarantees about how long that clairvoyance can last. It would be
unwise to follow its investment advice! As with the lucky coins, the
demon's powers are useless or unreliable. In fact, all the variants discussed in Hardin and
Taylor's
[monograph](https://qcpages.qc.cuny.edu/~rmiller/abstracts/Hardin-Taylor.pdf)
have some similar fineprint; this illustrates the fundamentally
nonconstructive nature of the AC, where we can build something, but we
can't say when, where, or for how long.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   You might ask <i>what</i> it's building. I think the simplest answer
	   is a sort of <a
	   href="https://en.wikipedia.org/wiki/Error_correction_code">error
	   correcting code</a>, robust against any finite (or countable,
	   or "scattered" as per Definition 7.1.1 of
	   [Hardin and Taylor](https://qcpages.qc.cuny.edu/~rmiller/abstracts/Hardin-Taylor.pdf))
	   number of errors. It is not useful since what it is good at
	   transmitting or predicting is the equivalence class
	   representative chosen by the AC, not the other points in the
	   equivalence class we care about for our application.
	   </span>
I think physics remains safe from Zermelo's demon.

### Further reading

- ["The Axiom of Choice and the No-Signaling Principle"](https://arxiv.org/abs/2206.08467)
(2023), Ämin Baumeler, Borivoje Dakić, and Flavio Del Santo.
- ["A Peculiar Connection between the Axiom of Choice and Predicting the Future"](https://www.jstor.org/stable/27642415)
  (2008), Christopher Hardin and Alan Taylor.
- [*The Mathematics of Coordinated Inference: A Study of Generalized Hat Problems*](https://qcpages.qc.cuny.edu/~rmiller/abstracts/Hardin-Taylor.pdf)
  (2013), Christopher Hardin and Alan Taylor.

## 3. <a href="#tbc">Why the sky is blue</a><a id="sec-3" name="sec-3"></a>

Why is the sky blue? The conventional answer is that light bounces off
air molecules, and high frequencies bounce more.
Blue has a high frequency, so voilà!
But actually, violet has a higher frequency, so why isn't the sky
violet?
It turns out the sun emits more blue light, so surely that does it?
Do the calculation and you find it doesn't: the sky should be green.
So what gives?
We are missing physiology. It is the eye, and not
the sky, that is blue.

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/sky1.svg" width="520"/>
	</div>
</figure>

To carefully determine the perceived colour, we need to go and look up
[luminous efficiency functions](https://en.wikipedia.org/wiki/Luminous_efficiency_function)
and do some integrals.
This is hard work, and I am lazy.
Luckily, there is a quick hack to get the right answer!
The idea is to assume the eye adapted to receive
sunlight, so we can take its sensitivity curve $V$ to match the spectral
intensity $I_{\odot}$ of the sun.
Direct sunlight makes up about $85\%$ of the
total power at midday, with the rest being the scattered or diffuse
light itself, of intensity
$I_{\text{diff}}$. So a better approximation is
$$
V = \alpha I_{\odot} + (1-\alpha) I_{\text{diff}}
$$
for $\alpha=0.85$.
Let's see what color this predicts!

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Spectral Adaptation.</span>
If the eye adapts to a mixture of direct and diffuse sunlight with
weight $\alpha$, the peak<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
To identify this with the color of the sky is a bit of a cheat, since
the “peak” of the spectrum depends on how we parametrize it. It's
different if we use wavelength!
We really need to integrate against curves for each cone's sensitivity curve.
	   </span> frequency is
\[
h\nu^* = (7 - 2 \alpha)(1- e^{2\alpha - 7}) k_\text{B}T_{\odot},
\]
where $h$ is Planck's constant, $k_\text{B}$ is Boltzmann's constant, and $T_\odot$ is the
sun's temperature. Setting $\alpha = 0.85$, this corresponds to a
wavelength of $\lambda^* = 473 \text{ nm}.$
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Calculation.</i>$\,$ (Summary: take a product of spectra, differentiate, and set to zero.)
  </summary>
  <p>
For frequency $\nu$ and temperature $T$, the blackbody spectrum is
proportional to
\[
I_{\odot}(\nu) \propto \frac{\nu^{3}}{e^{h\nu/k_\text{B}T_{\odot}} - 1} \propto \frac{x^3}{e^x-1}
\]
for a dimensionless variable $x = h\nu/k_\text{B}T_{\odot}$.
Rayleigh scattering increases with frequency as $\nu^{4}$, so
$I_{\text{diff}}(\nu) \propto \nu^4 I_{\odot}(\nu)$. Thus, for
weight $\alpha$, the sensitivity curve goes as
\[
V(\nu) = \alpha I_{\odot}(\nu) + (1- \alpha)I_{\text{diff}}(\nu) \propto \left[\alpha + (1-\alpha) x^4\right] \frac{x^3}{e^x - 1},
\]
since using the dimensionless variable ensures the constants of
proportionality are equal.
The perceived spectrum of the sky is $V\cdot I_{\text{diff}}$.
To find the peak of this spectrum, we differentiate with respect to $x$
and set to zero, yielding the equation
\[
(x - \beta)e^{x} +\beta = 0
\]
for $\beta = 7 - 2\alpha$.
This is transcendental, but has <a
href="https://hapax.github.io/physics/everyday/sky/">approximate
solution</a> $x \approx \beta(1- e^{-\beta})$. The corresponding frequency and wavelength
peaks are then
\[
h\nu^* = \beta (1- e^{-\beta}) k_\text{B}T_{\odot} \quad \Longrightarrow \quad
\lambda^* = \frac{c}{\nu^*} = \frac{hc}{\beta (1- e^{-\beta}) k_\text{B}T_{\odot}}.
\]
If we now plug in $hc/k_\text{B} = 14.39 \text{
mm K}$, $\alpha = 0.85$, and $T_{\odot} = 5772 \text{ K}$ for the
surface temperature of the sun, we get
\[
\lambda^* = \frac{14.39 \text{ mm}}{\beta (1- e^{-\beta}) \cdot 5772} \approx 473 \text{ nm},
\]
<!-- 14.39/((7-2 *0.85)(1-exp(-(7-2 *0.85)))5772) -->
as claimed.
$\blacksquare$
</p>
</details>
</div>

For reference, the real answer is $\lambda = 475 \text{ nm}$.
Although the adaptive hypothesis is wrong,<label for="sn-1"
       class="margin-toggle sidenote-number"></label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   The (photopic) luminous efficiency
	   function falls off much faster than a blackbody distribution.
	   The evolutionary hypothesis isn't bad, but ignores the
       physiological constraints on the eye which favor a narrowband design.
</span>
it produces an estimate astoundingly close to the real color of the
       sky—the mark of a good hack!
To get a sense of the discrepancy, we can
[map](https://www.luxalight.eu/en/cie-convertor) these frequencies to color space using the
[CIE 1931 model](https://en.wikipedia.org/wiki/CIE_1931_color_space), which
implements all that ocular physiology we ignore.
It's close, but a few nanometers can still make a perceptible difference.

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/sky4.svg" width="350"/>
	</div>
	</figure>

This concludes our discussion of daylight. But the sky
isn't only blue during the day; it is sometimes blue as twilight
falls, a fact I noticed a few weeks ago on an evening drive. The blue
lies below the pure black of the night sky, and above the orange and
yellow of sunset. The black makes sense because, far enough from the
horizon, all the light has been scattered.<label for="sn-1"
       class="margin-toggle sidenote-number"></label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   Of course, go further and the sky is dark simply because it is in the
	   [earth's shadow](https://en.wikipedia.org/wiki/Earth%27s_shadow).
	   </span>
Sunset is yellow and orange because most of the blue light has been
scattered out by the time it arrives at the eye. But the band in between is a puzzle!

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/sky2.svg" width="230"/>
	</div>
	</figure>

The explanation turns out to involve new physics: the
[absorption](https://en.wikipedia.org/wiki/Chappuis_absorption) of red
and yellow light by atmospheric ozone ($\text{O}_3$), a process discovered by James Chappuis in 1880
and applied to twilight by Edward Hulbert in the 1950s.
Ozone molecules absorb photons, get excited and start wobbling, then split
into oxygen molecules ($\text{O}_2$) and/or atoms ($\text{O}_1$).
Wobbling is a continuous phenomenon, leading to a band of frequencies
that can be absorbed.
However, there are two peaks, at $\lambda = 575 \text{ nm}$ and
$\lambda = 603 \text{ nm}$:

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/sky3.svg" width="350"/>
	</div>
	</figure>

No wonder the sunset colors disappear so quickly!
They're eaten up by the ozone, and what's left over is the blue light not yet scattered.
You might wonder if the hole in the ozone layer leads to less blue in
the antarctic twilight. As
[Lee, Meyer and Hoeppe](https://application.wiley-vch.de/books/sample/3527403205_c01.pdf)
show from painstaking observation at the south pole, it does
not, which raises yet more questions...<label for="sn-1"
       class="margin-toggle sidenote-number"></label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
       This paper has a wonderful potted history of the optics of
	   twilight. They conclude that, although ozone is the main factor, aerosol levels, haze, and high-altitude meteorology are
	   also relevant, particularly for the color of the zenith.
	   </span> With atmospheric optics, something is always up in the air.

### References

- [*Atmospheric Optics*](https://application.wiley-vch.de/books/sample/3527403205_c01.pdf)
  (2003), Craig Bohren.
- ["Atmospheric ozone and colors
of the Antarctic twilight sky"](https://web.archive.org/web/20230401024504/https://www.usna.edu/Users/oceano/raylee/papers/RLee_O3_twilights_paper.pdf)
(2011), Raymond Lee Jr., Wolfgang Meyer, and Götz Hoeppe.

## 4. <a href="#tbc">A game of primes</a> <a id="sec-4" name="sec-4"></a>

I'm working on a research problem where, the more prime factors a
number has, the harder the problem becomes. So I began to wonder: how many
prime factors does a large number have, on average? Suppose $n$ can be
factorized as

$$
n = p_1^{m_1} p_2^{m_2} \cdots p_{T}^{m_{T}},
$$

where each $p_j$ is prime, $m_j > 0$ and $T$ is the total number of
primes.
Define

$$
\omega(n) = T, \quad \Omega(n) = \sum_{j=1}^T m_j.
$$

We'll focus on the second function, and try to understand its
behaviour. A good place to start is numerics. Let's plot a histogram
of the distribution of $\Omega(n)$ in a window (of width $\min\\{n, 10^3\\}$)
around $n_a = 10^{2a}$, for integers $1 \leq a \leq 5$:

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/prime1.svg" width="760"/>
	</div>
	</figure>

We've marked the mode on the $x$ axis, and although the distribution
       spreads out to the right, it does so very, very slowly.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   As Carl Pomerance said of a related function, it “has been
	   proved to go to infinity but has never been observed to do so”.
	   </span>
The distribution itself looks well-behaved; after
inspecting a few skewed possibilities, the best visual match is the
[Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):

$$
X \sim \text{Poiss}(\lambda), \quad \mathbb{P}(X=k) = e^{-k}\frac{\lambda^k}{k!}.
$$

This distribution counts the number of times an event occurs, where each occurrence is
independent and the average number of occurrences is $\lambda$. It's
tempting to view a prime $p$ as being Poisson
distributed $\sim \text{Poiss}(\lambda_p)$ over each $n$, with $k$ counting how many times $p$ divides $n$.
Since a prime $p$ divides every $p$th numer, we expect that $\lambda_p
= 1/p$.
If these different prime factors are independent of each other, then
the total number of factors, on average, is

$$
\mathbb{E}_{m\leq n}[\Omega(m)] = \sum_{p\leq n} \lambda_p = \sum_{p\leq n} \frac{1}{p}.
$$

We can do this sum assuming a few results from analytic
number theory.

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Sum of Reciprocal Primes.</span>
The sum of reciprocal primes less than $n$ is
\[
\sum_{p\leq n} \frac{1}{p} \sim \log \log n,
\]
where $f(n) \sim g(n)$ means $\lim_{n\to\infty} f(n)/g(n)=1$.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: integrate by parts and use the prime
number theorem.)
  </summary>
  <p>
For a continuously differentiable function
$f(x)$ on $[\alpha, \beta]$, and a sequence $c_n$, we can integrate by
parts to obtain <a
href="https://en.wikipedia.org/wiki/Abel%27s_summation_formula">Abel's
<span
style="font-variant: small-caps">Partial
Summation Formula</span></a>
\[
	\sum_{\alpha \leq n \leq \beta} c_n f(n) = C(\beta)f(\beta) - \int_\alpha^\beta \text{d}x \, C(x)f'(x), \tag{1}\label{abel}
\]
where $C(x) = \sum_{\alpha \leq n\leq x} c_n$.
The <a href="https://en.wikipedia.org/wiki/Prime_number_theorem"><span
style="font-variant: small-caps">Prime Number Theorem</span></a>
(PNT) states that the number of primes less than $n$, $\pi(n)$, has
asymptotic approximation
\[
\pi(x) = \sum_{2 \leq n \leq x} \delta_{\mathbb{P}}(n) \sim \frac{x}{\log x} \tag{2}\label{PNT}
\]
where $\delta_{\mathbb{P}}(n)$ is the indicator function on
primes. Setting $c_n = \delta_{\mathbb{P}}(n)$, $f(n) = 1/n$, and
noting that $C(x) = \pi(x)$, Abel summation applied to $(\ref{PNT})$ gives
\begin{align*}
	\sum_{p \leq n} \frac{1}{p} & = \frac{\pi(n)}{n} + \int_2^n \text{d}x \,
	\frac{\pi(x)}{x^2} \sim \frac{1}{\log(n)} + \int_2^n \,\frac{\text{d}x}{x\log x}
    \sim \log \log n,
	 \end{align*}
discarding $1/\log n - \log\log 2 = o(\log\log n)$.
$\blacksquare$
  </p>
</details>
</div>

So, we expect that if we average $\Omega(m)$ over a large enough
interval $[2, n]$, we should obtain $\log \log n$.
In fact, a sum of independent Poisson-distributed variables is
Poisson, since Poisson just counts the total number of events in terms
of the expected number, and both the total and the expected number sum.
This means that

$$
	\Omega(n) \sim \sum_{p \leq n}\text{Poiss}\left(\frac{1}{p}\right) = \text{Poiss}(\log \log n).
$$
	   
Plotting the probability density over the top of the empirical curves,
we see excellent agreement, even at relatively small numbers! It looks as
if the primes are Poisson random.

<figure>
    <div style="text-align:center; padding: 5px"><img src
    ="/img/posts/prime4.svg" width="760"/>
	</div>
	</figure>

If true, this has a remarkable consequence. As $n$
becomes large, $\text{Poiss}(nx)$ can be viewed as a sum of $n$
i.i.d. variables $\text{Poiss}(x)$.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   Since, as we said, a sum of independent Poisson variables is Poisson.
	   </span> By the
[<span style="font-variant: small-caps">Central Limit Theorem</span>](https://en.wikipedia.org/wiki/Central_limit_theorem)
(CLT),
this will tend to a normal distribution $\mathcal{N}(\mu, \sigma^2)$
of mean and variance $\mu=\sigma^2=nx$. For our
prime divisor-counting function $\Omega(n)$, this suggests that, as $n \to \infty$,

$$
\Omega(m) \sim \mathcal{N}(\log\log n, \log\log n),
$$

when $m \leq n$. The number of factors is Gaussian!

<figure>
    <div style="text-align:center; padding: 10px"><img src
    ="/img/posts/prime3.svg" width="500"/>
	</div>
	</figure>

Let's turn now to what can be proven using cold, hard math. The PNT $(\ref{PNT})$
was rigorously derived in 1896 (by Hadamard and de la Vallée Poussin
independently) but in 1903, Edmund Landau gave a simpler proof and
made the following generalization:

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Poisson Primes.</span>
Let $\pi_k(n)$ count
the numbers $m$, $1 \leq m\leq n$ with $k$ prime factors (with multiplicity), i.e. such
that $\Omega(m) =
k$.
Then
\[
\pi_k(n) \sim \frac{n}{\log n}\frac{(\log\log n)^{k-1}}{(k-1)!}.
\tag{3} \label{PNT+}
\]
So $\Omega(m)$ for $m \leq n$ is asymptotically Poisson distributed with parameter $\lambda = \log\log n$.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: remove a prime from the set, use
induction, and do a horrible integral.)
  </summary>
  <p>
For a prime $p \leq n$, the number of sets of $k$ primes, which
produce to less than $n$, is given by the number of sets of $k-1$
which produce to less than $n/p$:

$$
\pi_{k}(n) = \sum_{p \leq} \pi_{k-1}\left(\frac{n}{p}\right).
$$
I'll give an original but not entirely rigorous argument from induction; for a more careful
proof, see <a
 href="https://archive.org/details/introductiontoth00hard/page/n5/mode/2up">Hardy
 and Wright</a> (Theorem 437).
The base case $k=1$ is the PNT $(\ref{PNT})$.
Assume it is true for $k$; we will use this to show $k+1$. Our goal is
to
count all sets of $k+1$ primes whose product is less than $n$. If
$p$ is in some of these sets, the remaining primes must multiply to
less than $n/p$. Hence,
\begin{align*}
\pi_{k+1}(n) = \sum_{p \leq n} \pi_{k}\left(\frac{n}{p}\right) = \sum_{p \leq n}
\frac{1}{(k-1)!}\frac{n/p}{\log(n/p)}[\log\log (n/p)]^{k-1}.
\end{align*}
Using Abel summation $(\ref{abel})$ in a similar fashion to earlier, we
can replace summing over primes with integration weighted by $1/\log x$:
\begin{align*}
\pi_{k+1}(n) & \sim \frac{1}{(k-1)!}\int_2^{n/2} \text{d}x\,
\frac{n/x}{\log x\log(n/x)}[\log\log (n/x)]^{k-1}\\
& = -\frac{1}{(k-1)!}\frac{n}{\log n}\int_{\log\log 2}^{\log\log (n/2)} \text{d}t\,
\frac{t^{k-1}}{e^t/\log n - 1},
\end{align*}
where we sum up to $n/2$ in the first integral<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">Since if $x > n/2$, then $n/x < 2$ and
	   hence $\pi_k(n/x) = 0$.
	   </span>
and make the substitution $t = \log \log (n/x)$ in the second.
This can be evaluated, using computer algebra or by hand, in terms of
the <a
href="https://en.wikipedia.org/wiki/Polylogarithm">polylogarithm</a> $\text{Li}_j(x)$.
Slogging through, we get
\begin{align*}
\pi_{k+1}(n) & \sim \frac{n}{\log n} \sum_{j =1}^{k}\frac{1}{(k-j)!}
\left[\text{Li}_{j}\left(\frac{\log n}{\log (n/2)}\right) - \text{Li}_{j}\left(\frac{\log n}{\log 2}\right)\right].
\end{align*}
The second term, at highest $j$, dominates, with
\[
\pi_{k+1}(n) \sim -\frac{n}{\log n} \text{Li}_{k}\left(\frac{\log n}{\log 2}\right)
\sim \frac{n}{\log n} \frac{(\log\log n)^k}{k!}
\]
from the large argument asymptotics of $\text{Li}_k$.
This is what we wanted to show!
$\blacksquare$
  </p>
</details>
</div>

Now that we have an asymptotic Poisson distribution, we can show that
at large $n$, the values of $\Omega(m)$ for $m \leq n$ are normally
distributed. The rigorous proof of this result is called the <span
style="font-variant: small-caps">Erdős–Kac Theorem</span>, and its genesis is legendary.
In Mark Kac's words:

<div style="padding: 5px 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<i>I first stated the conjecture during a lecture in Princeton in March 1939.
Fortunately for me and possibly for mathematics, Erdős was in the audience and he
immediately perked up. Before the lecture was over he had completed
the proof.</i><br>
</div>

Erdős and Kac's derivation uses complicated sieve methods, and although simpler proofs now exist, we won't
bother. Instead, I'll finish with the heuristic<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   It's heuristic because we haven't kept track of the errors in
	   Landau's formula for $\pi_k(n)$.
	   </span> argument Kac may have presented at
Princeton in 1939, since it uses Landau's Poisson primes results.
This is one of the many gems in
[Kac's book](https://store.doverpublications.com/0486821587.html) on probabilistic methods.

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">The Erdős–Kac Theorem.</span>
Let $K_n(\alpha, \beta)$ denote the
number of integers $1 \leq m\leq n$ such that
\[
\log \log n +\alpha\sqrt{\log \log n} <  \Omega(m) < \log \log n +\beta\sqrt{\log \log n}.
\]
Dividing $K_n(\alpha, \beta)$ by $n$ gives the empirical probability that $\Omega(m)$ is
within these bounds. Then
\[
\lim_{n\to\infty} \frac{1}{n}K_n(\alpha, \beta) = \frac{1}{\sqrt{2\pi}}\int_{\alpha}^\beta \text{d}t \, e^{-t^2/2},
\]
so that for $m \leq n$, $\Omega(m)$ is normally distributed with mean
and variance $\log\log n$.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
<i>Proof.</i>$\,$ (Summary: apply the CLT to Landau's Poisson distribution.)
  </summary>
  <p>
Let's start by stating the CLT more precisely for a Poisson variable
$X \sim \text{Poiss}(x)$. As $x \to \infty$, the CLT tells us that
$Y = (X - x)/\sqrt{x} \sim \mathcal{N}(0, 1)$, and hence
\[
\lim_{x\to \infty} \mathbb{P}_{X \sim \text{Poiss}}(x + \alpha\sqrt{x} < X <x + \beta\sqrt{x} ) =\mathbb{P}_{Y \sim \mathcal{N}}(\alpha < Y < \beta),
\]
or more explicitly,
\[
\lim_{x\to\infty} \sum_{x + \alpha\sqrt{x} < k < x+ \beta\sqrt{x}} e^{-x}\frac{x^k}{k!} =\frac{1}{\sqrt{2\pi}}\int_\alpha^\beta \text{d}t\, e^{-t^2/2}. \label{CLT} \tag{5}
\]
We can split the count of $\Omega(m)$ into values it assumes, i.e. the
number of $m \leq n$ with $\Omega(m)  = k$, which is given by $\pi_k(n)$.
Applying  $(\ref{CLT})$ to Landau's formula $(\ref{PNT+})$, with
$x=\log\log n$, we deduce
\begin{align*}
\lim_{n\to \infty}\frac{1}{n}K_n(\alpha, \beta) & = \lim_{n\to
\infty}\sum_{x + \alpha \sqrt{x} < k < x + \beta
\sqrt{x}} \frac{\pi_k(n)}{n} \\
& = \lim_{x\to\infty} \sum_{x + \alpha \sqrt{x} < k < x + \beta \sqrt{x}}
e^{-x}\frac{x^{k-1}}{(k-1)!} \\
& = \frac{1}{\sqrt{2\pi}}\int_\alpha^\beta \text{d}t\, e^{-t^2/2}.
\end{align*}
Thus, $K_n \sim \mathcal{N}(x, x)$, and the values of $\Omega(m)$ for
$m \leq n$ are normally distributed. $\blacksquare$
  </p>
</details>
</div>

Kac said, rather poetically, that the primes play a game of
chance. Landau's formula implies that they play the same
game as gum on the sidewalk, which is less poetic, but no less true.

### References

- [*An Introduction to the Theory of Numbers*](https://archive.org/details/introductiontoth00hard/page/n5/mode/2up) (1938), G. H. Hardy and
  E. M. Wright.
- [*Statistical Independence in Probability, Analysis and Number
  Theory*](https://store.doverpublications.com/0486821587.html) (1959), Mark Kac.

## 5. <a href="#tbc">The firefly effect</a> <a id="sec-5" name="sec-5"></a>

For a long time, I've been itching to make a firefly simulator, and
finally got round it! More specifically, I wanted to simulate the
famous
[phase synchronization](https://www.nps.gov/grsm/learn/nature/fireflies.htm)
of lightning bugs (*Photinus carolinus*) in the Great Smoky Mountains of North Carolina
and Tennessee. It happens during mating season, once a year, and is so
popular there is a lottery for park entrance permits.

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="https://cdn.recreation.gov/public/2019/02/05/20/53/233374_a7d56e6d-b229-43f2-905e-95dfc905298f_1440.jpg" width="600"/>
	</div>
	</figure>

The mathematics is also very beautiful.
If we think of the fireflies as oscillators, with brightness varying
sinusoidally, then some sort of interaction or *coupling* between
oscillators must cause them to synchronize.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	  The fireflies do more than oscillate at a single frequency, with
	  a "bright" period of $5$ to $8$ flashes followed roughly $8$
	  seconds of darkness. This isn't so hard to add, but we won't do
	  it here for simplicity.
	   </span>
Kuramoto (1975) introduced a cute model for coupled oscillators which
does the trick.
Say we have $N$ fireflies, $i=1, 2, \ldots, N$, with brightness
controlled by a phase $\theta_i$.
If firefly $i$ oscillates with natural frequency $\omega_i$, its phase
changes as

$$
\frac{\mathrm{d}\theta_i}{\mathrm{d}t}  = \dot{\theta}_i= \omega_i.
$$

To measure how synchronized our fireflies are, we
define the average phasor

$$
re^{i\psi} = \frac{1}{N}\sum_{j=1}^N e^{i\theta_j}
$$

with *coherence* $r$ and *mean phase* $\psi$.
Phases add constructively if $\theta_j = \theta$ is the
same for all $j$, with $r = 1$, but otherwise there is some
destructive intereference, and in the extreme case $r = 0$. We draw some cartoon phasors below:

<figure>
    <div style="text-align:center; padding: 10px"><img src
    ="/img/posts/firefly1.svg" width="400"/>
	</div>
	</figure>

Let's now think about how to couple the oscillating bugs.
If firefly $i$ aligns its phase with firefly $j$, then
$\theta_i$ will slow down if it is ahead of $\theta_j$
($\theta_j - \theta_i < 0$), and speed up if it lags behind
($\theta_j - \theta_i > 0$).
We can implement such a coupling by adding a term to $\dot{\theta}_i$
proportional to the difference of phases:

$$
\dot{\theta}_i = \omega_i + k (\theta_j - \theta_i),
$$

for a coupling constant $k > 0$.
This has the same mathematical form as friction. Unfortunately, it
doesn't respect the $2\pi$ periodicity of the phase.
But when $\theta_j \approx \theta_i$, $\sin(\theta_j - \theta_i)$
approximates a linear coupling, and this does respect
periodicity. Making this replacement, and summing over all fireflies,
we obtain the Kuramoto model:

$$
\dot{\theta}_i = \omega_i + \frac{K}{N}\sum_{j=1}^N\sin(\theta_j - \theta_i)
$$

for a normalized coupling constant $K = Nk$.
Below, we provide a firefly simulator, with natural frequencies $\omega_i$
uniformly randomized over $[-0.01, 0.01]$ and initial phases
$\theta_i$ over $[-\pi, \pi)$.<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
	   <span class="sidenote">
	   Fireflies also wander randomly, but this is for aesthetic effect only and unrelated to the flashing.
	   </span>
You can control the normalized coupling $K$ by clicking and dragging
the mouse up and down, and the number of fireflies $N$ with the left and
right arrow keys.

<div id="sketch-holder"></div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
<script src="/assets/fireflies_data/firefly1.js"></script>

At zero coupling, fireflies flash in an uncorrelated way and the
coherence vanishes. Above some critical threshold, we begin to see *partial
synchronization*, and eventually $r = 1$ and all fireflies blink in unison.
You can see this in the simulation. If you're interested, I've included details about the analytic solution in the large-$N$ limit below, following [Acebrón et al.](https://home.iscte-iul.pt/~jaats/myweb/papers/new_kuras.pdf)

<div style="background-color: #202229 ; padding: 20px; border: 0px solid
grey; line-height:1.5; border-radius: 15px">
<span style="font-variant: small-caps">Kuramoto Oscillators in the
Large-$N$ Limit.</span> Suppose natural frequencies have an even probability
distribution $g(\omega) = g(-\omega)$. As $N \to \infty$, there are two populations of firefly:
uncorrelated <i>wanderers</i> with $|\omega| > Kr$, and uniformly random phase; and <i>sycnhronizers</i> satisfying
\[
\frac{\omega}{K r} = \sin(\theta - \psi),
\]
where $re^{i\psi} = \sum_{j=1}^{N}e^{i\theta_j}$ is the phasor we
introduced above. We picture these below:

<figure>
    <div style="text-align:center; padding: 15px"><img src
    ="/img/posts/firefly2.svg" width="250"/>
	</div>
	</figure>

The partially synchronized followers only appear above a critical coupling
$K_c = 2/[\pi g(0)]$.
</div>

<div style="background-color: #16171c ; padding: 20px 20px 5px 20px; border: 0px solid
grey; line-height:1.5">
<details>
  <summary>
   <i>Calculation.</i>$\,$ (Summary: couple to the mean field, turn sums to integrals and
   solve a Liouville equation.)
  </summary>
  <p>
  
  At time $t$, our phasor can be written as a convolution over a
  discrete probability density:
  \[
  re^{i\psi} = \frac{1}{N}\sum_{j=1}^N e^{i\theta_j(t)} = \int_{-\pi}^\pi \mathrm{d}\theta\, e^{i\theta} \sum_{a=1}^A \frac{n_a}{N}\rho(\theta, t | \omega_a),
  \]
  where there is a set of $A$ "occupied" natural frequencies
  $\omega_a$, and $n_a$ oscillators with frequency $\omega_a$.
  We can use the phasor to implicitly couple oscillators as follows.
  Multiplying the phasor by $e^{-i\theta_i}$ and taking the imaginary
  part gives
  \[
  \Im[re^{i(\psi - \theta_i)}] = r\sin(\psi} - \theta_i) =
  \frac{1}{N}\sum_{j=1}^N \sin(\theta_j - \theta_i) =
  K^{-1}(\dot{\theta}_i - \omega_i) \quad \Longrightarrow \quad
  \dot{\theta}_i = \omega_i + Kr\sin(\psi} - \theta_i).
  \]
  In the $N \to \infty$ limit, we replace $n_a/N$ by the continuous distribution
  $g(\omega)$:
  \[
  re^{i\psi} = \int_{-\pi}^\pi \mathrm{d}\theta\, \int_{-\infty}^\infty \mathrm{d}\omega \, e^{i\theta} \rho(\theta, t | \omega) g(\omega).
  \]
  
  </p>
 </details>
 </div>

Note that the for our uniform $g \sim \mathcal{U}(-0.01, 0.01)$

### References

- ["The Kuramoto model"](https://home.iscte-iul.pt/~jaats/myweb/papers/new_kuras.pdf)
  (2005), Juan Acebrón, L. Bonilla, Conrad Pérez Vicente,
  Félix Ritort and Renato Spigler.
- ["Stability of Incoherence in a Population of Coupled Oscillators"](https://static1.squarespace.com/static/5436e695e4b07f1e91b30155/t/54e26467e4b0be2d4c9815d9/1424122983677/stability-of-incoherence-in-a-population-of-coupled-oscillators.pdf)
  (1991), Steven Strogatz and Renato Mirollo.
